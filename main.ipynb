{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Project Version 2 Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Information and Our Information\n",
    "### **Paper Title:** SeD Semantic-Aware Discriminator for Image Super-Resolution\n",
    "### **Paper Authors:** Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen\n",
    "### **Paper Description:** \n",
    "### Github Repository: [Link](https://github.com/YigitEkin/sed)\n",
    "\n",
    "In this work, researchers highlight the use of Generative Adversarial Networks (GANs) for image super-resolution tasks, particularly focusing on texture recovery. They note a limitation in existing methods where a single discriminator is employed to teach the super-resolution network the distribution of high-quality real-world images, leading to coarse learning and unexpected output. To address this, they introduce a Semantic-aware Discriminator (SeD), which incorporates image semantics to guide the network in learning fine-grained image distributions.\n",
    "\n",
    "The SeD leverages image semantics extracted from a trained semantic model, allowing the discriminator to discern real and fake images based on different semantic conditions. By integrating semantic features into the discriminator using spatial cross-attention modules, they aim to enhance the SR network's ability to generate more realistic and visually appealing images. The approach capitalizes on pretrained vision models and extensive datasets to enrich the understanding of image semantics and improve the fidelity of super-resolved images.\n",
    "\n",
    "\n",
    "Authors suggest that Vanilla Discriminators ignore the important semantics of the inputs, hence giving  semantic features of an image ( extracted via a pretrained network ), enables a better discriminator and hence a better feedback for the generator. The situation is demonstrated better on Figure 1, giving semantic features as condition enables the discriminator to specialize by finding boundaries within classes.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/fig1.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "\n",
    "A classical setup of Super Resolution GAN Network is as below. The generator network takes the low resolution image as input and produces a high resolution image. The discriminator takes the generated and the ground truth high resolution images and classifies as real or fake. In our setup, our generator takes 64x64 low resolution images and generate 256x256 high resolution images.\n",
    "\n",
    "<img src=\"img/sr_gan_setup.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "The proposed setup of the paper is as below, now the semantic feature maps of the ground truth high resolution images is also given as input to the discriminator. \n",
    "\n",
    "\n",
    "<img src=\"img/sed_gan_setup.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "Authors employ two discriminator types, a patch-based discriminator and a pixel-wise discriminator. \n",
    "The proposed architecture of the Patch-wise Semantic Aware Discriminator is shown below. \n",
    "\n",
    "\n",
    "<img src=\"img/patchwise_sed.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Patchwise SED</summary>\n",
    "\n",
    "  ```python\n",
    "class DownSampler(nn.Module):\n",
    "    # Downsamples 4 times in a conv, bn, leaky relu fashion that halves the spatial dimensions in each step and doubles the number of filters\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters * 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_filters * 4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_filters * 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchDiscriminatorWithSeD(nn.Module):\n",
    "    # PatchGAN discriminator with semantic-aware fusion blocks \n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        #First downsample the input size from 256x256 to 16x16 to match the semantic feature map size\n",
    "        self.downsampler = DownSampler(input_channels, num_filters)\n",
    "        #Use 3 semantic-aware fusion blocks to fuse the semantic feature maps with the downsampled input\n",
    "        self.semantic_aware_fusion_block1 = SemanticAwareFusionBlock()\n",
    "        self.semantic_aware_fusion_block2 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        self.semantic_aware_fusion_block3 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        #Final convolution to get the output\n",
    "        self.final_conv = nn.Conv2d(num_filters * 16, 1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, semantic_feature_maps, fs):\n",
    "        x = self.downsampler(fs)\n",
    "        x = self.semantic_aware_fusion_block1(semantic_feature_maps, x)\n",
    "        x = self.semantic_aware_fusion_block2(semantic_feature_maps, x)\n",
    "        x = self.semantic_aware_fusion_block3(semantic_feature_maps, x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "\n",
    "The discriminator has a specialized block called Semantic Aware Fusion Block. Semantic Aware Fusion Block takes the ground truth semantic features extracted by CLIP Feature Extractor, and applies cross attention between either ground truth or generated high resolution images as shown below. First the generated (or ground truth) feature maps are processed through normalization and self-attention mechanism , and then cross attention is applied. \n",
    "\n",
    "\n",
    "<img src=\"img/semantic_aware_fb.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Semantic Aware Fusion Block</summary>\n",
    "\n",
    "  ```python\n",
    "class SemanticAwareFusionBlock(nn.Module):\n",
    "    def __init__(self, channel_size_changer_input_nc=512):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, 1024) \n",
    "\n",
    "        self.channel_size_changer1 = nn.Conv2d(in_channels=channel_size_changer_input_nc, out_channels=128, kernel_size=1)\n",
    "        self.reduce_channels2 = nn.Conv2d(in_channels=1024, out_channels=128, kernel_size=1)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(128)\n",
    "        self.layer_norm_2 = nn.LayerNorm(128)\n",
    "        self.layer_norm_3 = nn.LayerNorm(128)\n",
    "\n",
    "        self.self_attention = SelfAttention(128, num_heads=1, dimensionality=128)\n",
    "        self.cross_attention = CrossAttention(128, heads=1, dim_head=128)\n",
    "\n",
    "        self.GeLU = nn.GELU()\n",
    "\n",
    "        #define 1x1 convolutions\n",
    "        self.increase_channels1 = nn.Conv2d(256, 1024, 1)\n",
    "\n",
    "    def forward(self, semantic_feature_maps, fs):\n",
    "        # fs ( or sh for generated) have shape batch, 3 x 16 x 16\n",
    "        #semantic feature maps  have shape batch x 1024 x 16 x 16\n",
    "        final_permute_height = semantic_feature_maps.shape[2]\n",
    "        final_permute_width = semantic_feature_maps.shape[3]\n",
    "        \n",
    "        #first handle S_h\n",
    "        semantic_feature_maps = self.group_norm(semantic_feature_maps)\n",
    "\n",
    "        #reduce the channel dimensions for the feature maps from 1024 to 128 for computation\n",
    "        semantic_feature_maps = self.reduce_channels2(semantic_feature_maps)\n",
    "\n",
    "\n",
    "        # Permute dimensions to rearrange the tensor\n",
    "        semantic_feature_maps = semantic_feature_maps.permute(0, 2, 3, 1).contiguous().view(semantic_feature_maps.size(0), -1, semantic_feature_maps.size(1))\n",
    "\n",
    "        #apply layer normalization\n",
    "        semantic_feature_maps = self.layer_norm_1(semantic_feature_maps)\n",
    "\n",
    "        #apply self attention\n",
    "        semantic_feature_maps = self.self_attention(semantic_feature_maps) #returned has shape 1,196,128 for now\n",
    "        #apply layer normalization\n",
    "        query = self.layer_norm_2(semantic_feature_maps)\n",
    "\n",
    "        #now handle fs or  sh\n",
    "        #reduce the channel dimensions for the sh\n",
    "\n",
    "        #make number of channels = 128 to be compatible with the semantic feature maps\n",
    "        fs = self.channel_size_changer1(fs)\n",
    "\n",
    "        #to use fs as residual, obtain a clone, \n",
    "        #note that gradient still accumulates in the original fs, so no problem\n",
    "        fs_residual = fs.clone()\n",
    "\n",
    "        #permute the dimensions\n",
    "        fs = fs.permute(0, 2, 3, 1).contiguous().view(fs.size(0), -1, fs.size(1))\n",
    "\n",
    "        #apply cross attention, query is the semantic feature maps and fs is the key and value\n",
    "        out = self.cross_attention(query, fs)\n",
    "\n",
    "        #apply layer normalization\n",
    "        out = self.layer_norm_3(out)\n",
    "\n",
    "        #apply GeLU\n",
    "        out = self.GeLU(out)\n",
    "\n",
    "        #permute the dimensions\n",
    "        out = out.permute(0,2,1).contiguous().view(out.size(0), -1, final_permute_height, final_permute_width)\n",
    "\n",
    "        #add the residual\n",
    "        output = torch.cat((out,fs_residual), dim=1)\n",
    "\n",
    "        #increase the channels back to 1024\n",
    "        output = self.increase_channels1(output)\n",
    "    \n",
    "        return output\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "The architecture of the CLIP Feature Extractor is demonstrated below. The CLIP Feature Extractor has normally 4 layers, but authors suggest using the outputs of third layer as going further causes loss of spatial information which is problematic while restoring a high resolution image. The architecture is shown below.\n",
    "\n",
    "\n",
    "<img src=\"img/clip_feature_extractor.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>CLIP Feature Extractor</summary>\n",
    "\n",
    "  ```python\n",
    "\n",
    "class CLIPRN50(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  \n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "        #add the openai-provided normalization\n",
    "        #https://github.com/jianjieluo/OpenAI-CLIP-Feature/blob/01269a8fceb540d3b6477b43177ea33845c9514c/clip/clip.py#L82C9-L82C92\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "        #load\n",
    "        self.ckpt_path = \"RN50\"\n",
    "        self.load_ckpt(self.ckpt_path)\n",
    "        self.freeze()\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = self.preprocess(x)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "The pixel-wise discriminator has U-Net architecture, which also employs the Semantic-Aware Fusion Block. The authors use both patch-based and pixel-wise based discriminators to demonstrate effectiveness of the Semantic-Aware Fusion Block. The architecture of the Pixel-wise Semantic Aware Discriminator is shown below.  \n",
    "\n",
    "<img src=\"img/pixelwise_sed.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>Pixelwise SED</summary>\n",
    "\n",
    "  ```python\n",
    "\n",
    "class DownSamplerPx(nn.Module):\n",
    "    #downsamples 4 times in a conv, bn, leaky relu fashion that halves the spatial dimensions in each step and doubles the number of filters\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        return x\n",
    "    \n",
    "class UNetPixelDiscriminatorwithSed(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_filters=64):\n",
    "        super(UNetPixelDiscriminatorwithSed, self).__init__()\n",
    "\n",
    "        #downsampler takes 256x256 images and downsamples to the 16x16\n",
    "        #to make dimensionality compatible with semantic feature maps\n",
    "        self.downsampler = DownSamplerPx(in_channels, num_filters)\n",
    "        \n",
    "        # Semantic Aware Fusion Blocks\n",
    "        self.semantic_aware_fusion_block1 = SemanticAwareFusionBlock(channel_size_changer_input_nc=64)\n",
    "        self.semantic_aware_fusion_block2 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        self.semantic_aware_fusion_block3 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        \n",
    "        self.upconv1 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)\n",
    "        self.upconv2 = nn.Conv2d(1024, 64, kernel_size=1, stride=1)\n",
    "        self.upconv3 = nn.Conv2d(64, 3, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "    def forward(self,semantic_feature_maps, fs):\n",
    "        x = self.downsampler(fs)\n",
    "        enc1 = self.semantic_aware_fusion_block1(semantic_feature_maps, x)\n",
    "        enc2 = self.semantic_aware_fusion_block2(semantic_feature_maps, enc1)\n",
    "        enc3 = self.semantic_aware_fusion_block3(semantic_feature_maps, enc2)\n",
    "        \n",
    "        dec = self.upconv1(enc3 + enc2)\n",
    "        dec = self.upconv2(dec + enc1)\n",
    "        dec = self.upconv3(dec + x)\n",
    "        \n",
    "        return dec\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Throughout the experiments, we use the RRDB Generator proposed in ESRGAN paper, whose building blocks are shown in below Figure.\n",
    "\n",
    "<img src=\"img/rrdb_generator.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>RRDB Generator</summary>\n",
    "\n",
    "  ```python\n",
    "class DenseBlock(nn.Module):\n",
    "  '''\n",
    "  Dense Block structure from https://arxiv.org/pdf/1809.00219 Fig4 : Left\n",
    "  '''\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=5, is_upsample=False):\n",
    "        super().__init__()\n",
    "        self.blocks = make_blocks(in_channels, out_channels, num_blocks, is_upsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prev_features = x\n",
    "        for block in self.blocks:\n",
    "            current_output = block(prev_features)\n",
    "            prev_features = torch.cat([prev_features, current_output], dim=1)\n",
    "        return x + current_output * 0.2\n",
    "\n",
    "class Residual_in_ResidualBlock(nn.Module):\n",
    "  '''\n",
    "  RRDB  structure from https://arxiv.org/pdf/1809.00219 Fig4 : Right\n",
    "  consists of 3 Dense Blocks\n",
    "  '''\n",
    "    def __init__(self, in_channels, num_blocks=3, is_upsample=False):\n",
    "        super().__init__()\n",
    "        self.rrdb1 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        self.rrdb2 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        self.rrdb3 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.rrdb1(x)\n",
    "        out2 = self.rrdb2(out1)\n",
    "        out3 = self.rrdb3(out2)\n",
    "        return x + out3 * 0.2\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    '''ESRGAN Generator, which consists of 23 Residual in Residual Dense Blocks\n",
    "    paper : https://arxiv.org/pdf/1809.00219\n",
    "    '''\n",
    "    def __init__(self, in_channels=3, num_channels=64, num_blocks=23, clip_output=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_layer(in_channels, num_channels)\n",
    "        self.conv2 = get_layer(num_channels, num_channels)\n",
    "        self.conv3 = get_layer(num_channels, num_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.output = get_layer(num_channels, in_channels)\n",
    "        self.first_ups = get_layer(num_channels, num_channels, is_upsample=True)\n",
    "        self.second_ups = get_layer(num_channels, num_channels, is_upsample=True)\n",
    "        self.rrdb = nn.Sequential(*[Residual_in_ResidualBlock(num_channels) for _ in range(num_blocks)])\n",
    "        self.clip_output = clip_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv1(x)\n",
    "        x = self.rrdb(res)\n",
    "        x = self.conv2(x)\n",
    "        x = x + res\n",
    "        x = self.first_ups(x)\n",
    "        x = self.second_ups(x)\n",
    "        x = self.act(self.conv3(x))\n",
    "        if self.clip_output:\n",
    "            x = self.output(x).clip(-1, 1)\n",
    "        else:\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "         \n",
    "To see the effect of Semantic Aware Fusion Block , we also implemented Vanilla Patch-wise Discriminator and Vanilla Pixel-wise Discriminator.\n",
    "\n",
    "<details>\n",
    "  <summary>Patch-wise Discriminator</summary>\n",
    "\n",
    "  ```python\n",
    "#Vanilla patchgan discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        #Downsample the input size from 256x256 to 16x16\n",
    "        self.downsampler = DownSampler(input_channels, num_filters)\n",
    "        self.final_conv = nn.Conv2d(num_filters * 8, 1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, fs):\n",
    "        fs = self.downsampler(fs)\n",
    "        fs = self.final_conv(fs)\n",
    "        return fs\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Pixel-wise Discriminator </summary>\n",
    "\n",
    "  ```python\n",
    "class UNetPixelDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_filters=64):\n",
    "        super(UNetPixelDiscriminator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            self._conv_block(in_channels, num_filters),\n",
    "            self._conv_block(num_filters, num_filters),\n",
    "            self._conv_block(num_filters, num_filters * 2),\n",
    "            self._conv_block(num_filters * 2, num_filters * 4),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(num_filters * 4, num_filters * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            self._upconv_block(num_filters * 4, num_filters * 4),\n",
    "            self._upconv_block(num_filters * 4, num_filters * 2),\n",
    "            self._upconv_block(num_filters * 2, num_filters),\n",
    "            self._upconv_block(num_filters, num_filters),\n",
    "            nn.Conv2d(num_filters, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def _upconv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, fs):\n",
    "        # Encoder\n",
    "        # fs = 64\n",
    "        enc1 = self.encoder[0](fs) # 32x32x64\n",
    "        enc2 = self.encoder[1](enc1) # 16x16x64\n",
    "        enc3 = self.encoder[2](enc2) # 8x8x128\n",
    "        enc4 = self.encoder[3](enc3) # 4x4x256\n",
    "\n",
    "        #Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4) # 2x2x256\n",
    "\n",
    "        #Decoder with skip connections using addition\n",
    "        dec = self.decoder[0](bottleneck) # 4x4x256\n",
    "        dec = self.decoder[1](dec + enc4) # 8x8x128\n",
    "        dec = self.decoder[2](dec + enc3) # 16x16x64\n",
    "        dec = self.decoder[3](dec + enc2) # 32x32x64\n",
    "        dec = self.decoder[4](dec + enc1) # 64x64x1\n",
    "\n",
    "        return dec\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "### **Authors:**  Yigit Ekin and Mustafa Utku Aydogdu\n",
    "### **Mail:** e270207@metu.edu.tr e270206@metu.edu.tr\n",
    "\n",
    "### **Our Assumptions:**\n",
    "Implementing a super resolution model based solely on a paper, without access to the accompanying code, was challenging due to the complexities of understanding and implementing the loss function, architecture, and performance metrics described in the paper. Dealing with dimensionality inconsistencies in paper. Some are listed below.\n",
    "\n",
    "* we assumed that the group normalization has 32 groups (not stated in the paper)\n",
    "* we assumed that the conv block in patchwise discriminator is a  convolution block that doubles the channel size and with kernel_size of 4, stride=2 and padding=1 followed by a batch normalization block followed by a leaky relu block (not included in the last convolution block) which is not stated in the paper.\n",
    "* They did not specified the adverserial loss function details. As a result, we have decided to go with wassertein loss with gradient penalty to achieve a more stable training.\n",
    "* They did not specify how they have preprocessed the dataset. As a result, due to small number of images in the dataset, we have decided to conduct a literature survey on how different models have overcome this issue and found that ESRGAN does combine 2 datasets and crops random patches from each image to increase the number of images.\n",
    "* We have decided to move with crop size of 400 for hr images and 100 for lr images. This means that during training our model inputs 100x100 crops and tries to generate 400x400 hr version of it.\n",
    "* For cross attention, we have decided to use single head attention rather than multi head attention\n",
    "* CLIP preprocessor normally downscales the image to 224x224 before extracting embeddings. We believed that this can downgrade the performance w.r.t hr images as a result, we did not use this preprocessor.\n",
    "* To obtain same spatial dimensionality with the clip embeddings (for concatenation specified in the image below in part d), we added extra convolution layer that did not change the channel size but decreases the spatial dimensions.\n",
    "* The authors did not describe the weight (lambda) values of the loss functions as a result, we have decided to go with 1 for mse and 10 for gradient penalty in wasserstein loss\n",
    "* The authors did not specify whether they have used multi-head attention or single head attention. As a result, we have decided to go with single head attention because we thought it should be sufficient enough.\n",
    "* The authors did not specify the dimensionlity of attention head. So, we have decided to go with 128 as this will result in 8 times less memory usage. \n",
    "* For the coefficients of the losses (i.e VGG, adverserial, MSE), we have conducted several experiments and the current setup in the config files are the ones that have achieved the best scores. One thing that we have tried to keep constant is the ratio between the losses. For example, if the VGG loss is 0.1 times the adverserial loss, we have tried to keep this ratio constant in all experiments by changing the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters of your model\n",
    "\n",
    "We aim to compare the effect of SeD discriminator with vanilla discriminator. As a result, we have two different training setups. Before reading the hyperparameters, please note that the hyperparameters are the same for both models except for the discriminator part. In addition, the losses used for the model can be seen from the image below where L_s is VGG  perceptual loss, L_p is the pixelwise MSE loss and L_adv is the adverserial loss.\n",
    "\n",
    "\n",
    "<img src=\"img/losses.png\"> <br/> <br/>\n",
    "The hyperparameters of the models are as follows:\n",
    "\n",
    "### Vanilla Discriminator\n",
    "- **Accelerator**: 'gpu'\n",
    "- **Device**: 'cuda'\n",
    "- **PL Trainer**:\n",
    "  - `max_epochs`: 1000\n",
    "  - `accelerator`: 'gpu'\n",
    "  - `log_every_n_steps`: 50\n",
    "  - `strategy`: DDPStrategy(find_unused_parameters=True)\n",
    "  - `devices`: Number of available CUDA devices (determined by `torch.cuda.device_count()`)\n",
    "  - `sync_batchnorm`: True\n",
    "- **Train Batch Size**: 16\n",
    "- **Validation Batch Size**: 8\n",
    "- **Test Batch Size**: 8\n",
    "- **Image Size**: 256\n",
    "- **Dataset Module**:\n",
    "  - `num_workers`: 4\n",
    "  - `train_batch_size`: 16\n",
    "  - `val_batch_size`: 8\n",
    "  - `test_batch_size`: 8\n",
    "  - **Train Dataset Configuration**:\n",
    "    - `image_size`: 256\n",
    "    - `image_dir_hr`: \"data/dataset_cropped/hr\"\n",
    "    - `image_dir_lr`: \"data/dataset_cropped/lr\"\n",
    "    - `downsample_factor`: 4 (downsampling factor for low-resolution images)\n",
    "    - `mirror_augment_prob`: 0.5 (probability of applying mirroring w.r.t. y axis as a data augmentation)\n",
    "  - **Validation Dataset Configuration**:\n",
    "    - `image_size`: 256\n",
    "    - `image_dir_hr`: \"data/evaluation/hr/manga109\"\n",
    "    - `image_dir_lr`: \"data/evaluation/lr/manga109\"\n",
    "  - **Test Dataset Configuration**:\n",
    "    - `image_size`: 256\n",
    "    - `image_dir_hr`: \"data/evaluation/hr/manga109\"\n",
    "    - `image_dir_lr`: \"data/evaluation/lr/manga109\"\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 5e-5 \n",
    "    - `model_config`:\n",
    "      - `path`: \"pretrained_models/vgg16.pth\"\n",
    "      - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "      - `resize_input`: False\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1.0\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5\n",
    "  - `clip_generator_outputs`: False (whether to clip generator outputs to valid pixel range [-1,1])\n",
    "  - `use_sed_discriminator`: False (whether to use SeD discriminator)\n",
    "\n",
    "### SeD Discriminator\n",
    "- **Accelerator**: 'gpu'\n",
    "- **Device**: 'cuda'\n",
    "- **PL Trainer**:\n",
    "  - `max_epochs`: 1000\n",
    "  - `accelerator`: 'gpu'\n",
    "  - `log_every_n_steps`: 50\n",
    "  - `strategy`: DDPStrategy(find_unused_parameters=True)\n",
    "  - `devices`: Number of available CUDA devices (determined by `torch.cuda.device_count()`)\n",
    "  - `sync_batchnorm`: True\n",
    "- **Train Batch Size**: 16\n",
    "- **Validation Batch Size**: 8\n",
    "- **Test Batch Size**: 8\n",
    "- **Image Size**: 256\n",
    "- **Dataset Module**:\n",
    "  - `num_workers`: 4\n",
    "  - `train_batch_size`: 16\n",
    "  - `val_batch_size`: 8\n",
    "  - `test_batch_size`: 8\n",
    "  - **Train Dataset Configuration**:\n",
    "    - `image_size`: 256\n",
    "    - `image_dir_hr`: \"data/dataset_cropped/hr\"\n",
    "    - `image_dir_lr`: \"data/dataset_cropped/lr\"\n",
    "    - `downsample_factor`: 4\n",
    "    - `mirror_augment_prob`: 0.5 (probability of applying mirroring w.r.t. y axis as a data augmentation)\n",
    "  - **Validation and Test Dataset Configuration**:\n",
    "    - `image_size`: 256\n",
    "    - `image_dir_hr`: \"data/evaluation/hr/manga109\"\n",
    "    - `image_dir_lr`: \"data/evaluation/lr/manga109\"\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 5e-5\n",
    "    - `model_config`:\n",
    "      - `path`: \"pretrained_models/vgg16.pth\"\n",
    "      - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "      - `resize_input`: False\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1.0\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5\n",
    "  - `clip_generator_outputs`: False (whether to clip generator outputs to valid pixel range [-1,1])\n",
    "  - `use_sed_discriminator`: True (whether to use SeD discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with SeD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPORTANT NOTE:** the training of the model is done on a remote server where we have not used jupyter notebook. Normally, scripts in the first 3 cells are used to train the model. However, in order to not overly crowd the jupyter notebook for the reviewers, we have included the code that is responsible for training but the training logs will be displayed in the last cell of this section named as training loop which abstracts all this logic\n",
    "\n",
    "PLEASE DO NOT CHANGE THE FILE STRUCTURE THAT THE SUBMISSION HAS PROVIDED. THIS CAN CAUSE ERRORS IN THE TRAINING OF THE MODEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING CONFIG\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "accelerator = 'gpu'\n",
    "device = torch.device(\"cuda\") if accelerator==\"gpu\" else torch.device(\"cpu\")\n",
    "if accelerator == 'cpu':\n",
    "    pl_trainer = dict(max_epochs=1000, accelerator=accelerator, log_every_n_steps=50, strategy=DDPStrategy(find_unused_parameters=True), devices=1, sync_batchnorm=True) # CHECK sync_batchnorm in this and below part !!!\n",
    "else:\n",
    "    pl_trainer = dict(max_epochs=1000, accelerator=accelerator, log_every_n_steps=50, strategy=DDPStrategy(find_unused_parameters=True), devices=torch.cuda.device_count(), sync_batchnorm=True)  # CHECK strategy and find_unused_parameters!!!\n",
    "\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "###########################\n",
    "##### Dataset Configs #####\n",
    "###########################\n",
    "\n",
    "dataset_module = dict(\n",
    "    num_workers=4,\n",
    "    train_batch_size=train_batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    train_dataset_config=dict(image_size=256, image_dir_hr=\"data/dataset_cropped/hr\", image_dir_lr=\"data/dataset_cropped/lr\", downsample_factor=4,mirror_augment_prob=0.5),\n",
    "    val_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    "    test_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    ")\n",
    "\n",
    "##################\n",
    "##### Losses #####\n",
    "##################\n",
    "vgg_ckpt_path=\"pretrained_models/vgg16.pth\"\n",
    "loss_dict = dict(\n",
    "    VGG=dict(weight=5e-5, model_config=dict(path=vgg_ckpt_path, output_layer_idx=23, resize_input=False)),\n",
    "    Adversarial_G=dict(weight=1.0),\n",
    "    MSE=dict(weight=1.0),\n",
    "    Adversarial_D=dict(r1_gamma=10.0, r2_gamma=0.0)\n",
    ")\n",
    "\n",
    "#########################\n",
    "##### Model Configs #####\n",
    "#########################\n",
    "\n",
    "super_resolution_module_config = dict(loss_dict=loss_dict, \n",
    "    generator_learning_rate=1e-4, discriminator_learning_rate=1e-5, \n",
    "    generator_decay_steps=[50_000, 100_000, 150_000, 200_000, 250_000], \n",
    "    discriminator_decay_steps=[50_000, 100_000, 150_000, 200_000, 250_000], \n",
    "    generator_decay_gamma=0.5, discriminator_decay_gamma=0.5,\n",
    "    clip_generator_outputs=False,\n",
    "    use_sed_discriminator=True)\n",
    "\n",
    "#######################\n",
    "###### Callbacks ######\n",
    "#######################\n",
    "\n",
    "ckpt_callback = dict(every_n_train_steps=4000, save_top_k=1, save_last=True, monitor='fid_test', mode='min')\n",
    "synthesize_callback_train = dict(num_samples=12, eval_every=2000) # TODO: 4000\n",
    "synthesize_callback_test = dict(num_samples=6, eval_every=2000)\n",
    "fid_callback = dict(eval_every=4000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYTORCH LIGHTNING ALLOWS US TO USE CALLBACK FUNCTIONS DURING TRAINING. HENCE, WE HAVE USED CALLBACKS TO SAVE WEIGHTS OF THE MODEL. THE CALLBACK IS THE FOLLOWING:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the following Models:\n",
    "- Patch-wise Discriminator with SeD\n",
    "- Vanilla Patch-wise Discriminator\n",
    "- Pixel-wise Discriminator with SeD\n",
    "- Vanilla Pixel-wise Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | generator     | RRDBNet                   | 15.4 M\n",
      "1 | discriminator | PatchDiscriminatorWithSeD | 4.7 M \n",
      "2 | clip          | CLIPRN50                  | 23.4 M\n",
      "------------------------------------------------------------\n",
      "20.1 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "43.5 M    Total params\n",
      "173.867   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:39,  3.28s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:15,  1.41s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:08,  1.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:04,  1.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:02,  3.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  4.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:04<00:00,  6.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  6.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  7.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.85it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.531, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.02it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.07it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.15it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:20<00:00,  1.30it/s, v_num=0, lpips_test=0.241, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  8.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.215, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.57it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.203, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.86it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.20it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.194, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.01it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.184, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.89it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.55it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.179, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.80it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.30it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.25it/s, v_num=0, lpips_test=0.174, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:21<00:00,  1.29it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.22it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.170, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:12,  1.00s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  2.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.55it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.85it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.164, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.34it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.38it/s]\u001b[A\n",
      "Epoch 100:  29%|▎| 8/28 [00:06<00:16,  1.20it/s, v_num=0, lpips_test=0.163, VGG_^C\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/patchgan_sed.py\" #Training of patchgan discriminator with SeD\n",
    "\n",
    "!python train.py --config_file=$CFG  # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | generator     | RRDBNet            | 15.4 M\n",
      "1 | discriminator | PatchDiscriminator | 2.8 M \n",
      "2 | clip          | CLIPRN50           | 23.4 M\n",
      "-----------------------------------------------------\n",
      "18.2 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.180   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:42,  3.55s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:16,  1.52s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:08,  1.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:04<00:01,  3.50it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:04<00:01,  4.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:04<00:00,  4.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  5.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  6.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.70it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.539, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  7.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.77it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.252, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.24it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.220, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:13,  1.12s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.46it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.205, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.35it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.194, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.41it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.89it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.182, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  2.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.40it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.60it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.50it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.90it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.176, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.59it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  7.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  7.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  8.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.67it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.30it/s, v_num=0, lpips_test=0.169, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.169, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  6.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  7.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.64it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.165, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.162, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.51it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.03it/s]\u001b[A\n",
      "Epoch 102:  64%|▋| 18/28 [00:13<00:07,  1.34it/s, v_num=0, lpips_test=0.159, VGG^C\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Epoch 102:  64%|▋| 18/28 [00:13<00:07,  1.33it/s, v_num=0, lpips_test=0.159, VGG\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/patchgan.py\" #Training of patchgan discriminator without SeD\n",
    "\n",
    "!python train.py --config_file=$CFG #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                          | Params\n",
      "----------------------------------------------------------------\n",
      "0 | generator     | RRDBNet                       | 15.4 M\n",
      "1 | discriminator | UNetPixelDiscriminatorwithSed | 3.2 M \n",
      "2 | clip          | CLIPRN50                      | 23.4 M\n",
      "----------------------------------------------------------------\n",
      "18.6 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "42.0 M    Total params\n",
      "167.802   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:36,  3.01s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:14,  1.31s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:07,  1.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:04,  2.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:02,  2.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:01,  3.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  4.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:03<00:00,  5.63it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:03<00:00,  6.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:03<00:00,  7.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  7.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  3.04it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.578, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  8.59it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.09it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.243, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.31it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:19<00:00,  1.35it/s, v_num=0, lpips_test=0.215, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.86it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.51it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  8.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.77it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.202, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  8.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.78it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.192, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:14,  1.22s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:06,  1.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  3.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:02<00:02,  2.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:02<00:01,  3.79it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:02<00:01,  4.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:02<00:00,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  4.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  4.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  5.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:03<00:00,  4.14it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.185, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.66it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.180, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.47it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.32it/s, v_num=0, lpips_test=0.174, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.58it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.171, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:16,  1.37s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:06,  1.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.60it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  3.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:02<00:02,  2.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:02<00:01,  3.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:02<00:01,  4.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:02<00:00,  5.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  5.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  5.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  6.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:03<00:00,  4.10it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.164, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.55it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.45it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.09it/s]\u001b[A\n",
      "Epoch 107:   0%| | 0/28 [00:00<?, ?it/s, v_num=0, lpips_test=0.160, VGG_step=6.4^C\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/pixelwise_sed.py\" #Training of pixelwise discriminator with SeD\n",
    "\n",
    "!python train.py --config_file=$CFG #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | generator     | RRDBNet                | 15.4 M\n",
      "1 | discriminator | UNetPixelDiscriminator | 3.5 M \n",
      "2 | clip          | CLIPRN50               | 23.4 M\n",
      "---------------------------------------------------------\n",
      "19.0 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "42.3 M    Total params\n",
      "169.295   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:37,  3.10s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:14,  1.34s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:05,  1.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:02,  3.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  3.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:03<00:00,  5.47it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  6.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.98it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.532, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.30it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:22<00:00,  1.20it/s, v_num=0, lpips_test=0.238, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.57it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.216, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.63it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:22<00:00,  1.21it/s, v_num=0, lpips_test=0.201, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:13,  1.11s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  7.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  8.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  8.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.38it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.189, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.77it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.55it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.182, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.54it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.95it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.34it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.58it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:10<00:13,  1.17it/s, v_num=0, lpips_test=0.167, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:22<00:00,  1.21it/s, v_num=0, lpips_test=0.167, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:14,  1.17s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.07it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  8.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.44it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.166, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.77it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.80it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.39it/s]\u001b[A\n",
      "Epoch 99:  39%|▍| 11/28 [00:09<00:14,  1.17it/s, v_num=0, lpips_test=0.161, VGG_"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/pixelwise.py\" #Training of pixelwise discriminator without SeD\n",
    "\n",
    "!python train.py --config_file=$CFG #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><h3>Training Details:</h3></summary>\n",
    "\n",
    "  <details>\n",
    "      <summary><h4>Patchgan SeD:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_l_patchgan_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_patchgan_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_l_patchgan_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_l_patchgan_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_patchgan_sed.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "\n",
    "  <details>\n",
    "      <summary><h4>Vanilla Patchgan:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_l_patchgan.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_patchgan.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_l_patchgan.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_l_patchgan.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_patchgan.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "  <details>\n",
    "      <summary><h4>Pixelwise SeD:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_l_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_l_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_l_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_px_sed.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "  <details>\n",
    "      <summary><h4>Vanilla Pixelwise Discriminator:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_l_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_l_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_l_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_px.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained model and computing qualitative samples/outputs from that model.\n",
    "\n",
    "### **IMPORTANT! loss curves and outputs of the training process is logged under the directory /logs during training loop in order to display the results, one can execute the command below at the terminal in order to display the results**\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=logs/<experiment_name_under_the_directory>\n",
    "```\n",
    "\n",
    "### To make reviewer's job easier, we have provided the code needed to load a pretrained model and compute qualitative samples from the model but added the tensorboard logs from the training loop that had been executed from the training cells from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FID on SR images: 100%|██████████| 6/6 [00:04<00:00,  1.44it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 6/6 [00:00<00:00,  7.53it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 6/6 [00:00<00:00,  7.95it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 6/6 [00:00<00:00,  7.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from models.super_resolution_module import SuperResolutionModule\n",
    "from datasets.dataset_module import DatasetModule\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def postprocess_image(image, min_val=-1.0, max_val=1.0):\n",
    "    image = image.astype(np.float64)\n",
    "    image = np.clip(image, -1, 1)\n",
    "    image = (image - min_val) * 255 / (max_val - min_val)\n",
    "    image = image.astype(np.uint8)\n",
    "    # image = np.clip(image + 0.5, 0, 255).astype(np.uint8)\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    return image\n",
    "\n",
    "def generate_results(ckpt, image_dir_hr, image_dir_lr, save_path):\n",
    "    model = SuperResolutionModule.load_from_checkpoint(ckpt) \n",
    "    train_batch_size = 1  # given so that each image is processed by itself\n",
    "    val_batch_size = 1 # given so that each image is processed by itself\n",
    "    test_batch_size = 1 # given so that each image is processed by itself\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset_module = dict(\n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr, downsample_factor=4),\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr),\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr),\n",
    "    )\n",
    "\n",
    "\n",
    "    data_module_gt = DatasetModule(**dataset_module)\n",
    "    data_module_gt.setup('test')\n",
    "    dataloader = data_module_gt.test_dataloader()\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(f\"results/{save_path}\", exist_ok=True)\n",
    "    cnt = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Calculating FID on SR images\", total=len(dataloader)):\n",
    "        sr_images = model.make_high_resolution(batch)\n",
    "        sr_images = sr_images ['generated_super_resolution_image'].to(device)\n",
    "        #save the sr images to the \"sr_pngs\" folder\n",
    "        for i in range(len(sr_images)):\n",
    "            img = sr_images[i]\n",
    "            \n",
    "            img = postprocess_image(img.detach().cpu().numpy())\n",
    "            img = Image.fromarray(img)\n",
    "            img.save(f\"results/{save_path}/{cnt}.png\")\n",
    "            cnt += 1\n",
    "    \n",
    "torch.manual_seed(1256)\n",
    "np.random.seed(1256)\n",
    "ckpt=\"logs/2024-05-28_16-19-08_patchgan_sed/checkpoint/last.ckpt\"\n",
    "ckpt2=\"logs/2024-05-28_16-10-08_pixelwise_sed/checkpoint/last.ckpt\"\n",
    "ckpt3=\"logs/2024-05-28_16-12-06_pixelwise/checkpoint/last.ckpt\"\n",
    "ckpt4=\"logs/2024-05-28_16-17-05_patchgan/checkpoint/last.ckpt\"\n",
    "\n",
    "generate_results_dict = {\n",
    "    \"patchgan\": ckpt4,\n",
    "    \"patchgan_sed\": ckpt,\n",
    "    \"pixelwise\": ckpt3,\n",
    "    \"pixelwise_sed\": ckpt2,\n",
    "} \n",
    "\n",
    "image_path_hr = \"/kuacc/users/hpc-yekin/hpc_run/sed/test_images_fig3/downscaled\"\n",
    "image_path_lr = \"/kuacc/users/hpc-yekin/hpc_run/sed/test_images_fig3/downscaled\"\n",
    "\n",
    "for key, item in generate_results_dict.items():\n",
    "    generate_results(item, image_path_hr, image_path_lr, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and logs of training SeD patchwise discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and logs of training patchwise discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing results\n",
    "\n",
    "as explained in the goals.txt an updated version will be provided with the trained model weights as soon as possible. The incident that happened is known by gökberk hoca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating metrics for patchgan/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 55/55 [00:01<00:00, 41.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lpips:  0.6864215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating ssim SSIM on sr images: 100%|██████████| 55/55 [00:01<00:00, 42.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssim:  0.37364906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 55/55 [00:01<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psnr:  7.709990440715443\n",
      "calculating metrics for patchgansed/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "StarGAN v2\n",
    "Copyright (c) 2020-present NAVER Corp.\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "4.0 International License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
    "\"\"\"\n",
    "\n",
    "#WE HAVE IMPLEMENTED THIS CODE BLOCK BY USING THE REFERENCE AT THE TOP AS A GUIDANCE\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets.dataset_module import DatasetModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from losses.lpips.lpips import LPIPS\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def print_metrics_given_path(path):\n",
    "    print(\"calculating metrics for \" + path)\n",
    "    train_batch_size = 2  # given as temporary data\n",
    "    val_batch_size = 2 # given as temporary data\n",
    "    test_batch_size = 2 # given as temporary data\n",
    "    \n",
    "    \n",
    "    ################ lpips\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lpips_model = LPIPS(net_type='alex', device=device).to('cpu')\n",
    "    lpips_model.eval()\n",
    "    image_size = 256\n",
    "    dataset_module_gt = dict(\n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=\"data/dataset_cropped/hr\", image_dir_lr=\"data/dataset_cropped/lr\", downsample_factor=4,mirror_augment_prob=0),\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    "    )\n",
    "    \n",
    "    dataset_module_gt = DatasetModule(**dataset_module_gt)\n",
    "    dataset_module_gt.setup('test')\n",
    "    first_dataloader = dataset_module_gt.test_dataloader()\n",
    "    \n",
    "    \n",
    "    dataset_module_sr = dict( #UPDATE DIRS\n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=\"data/dataset_cropped/hr\", image_dir_lr=\"data/dataset_cropped/lr\", downsample_factor=4,mirror_augment_prob=0),\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=path, image_dir_lr=\"data/evaluation/lr/manga109/\"),\n",
    "    )\n",
    "    \n",
    "    data_module_sr = DatasetModule(**dataset_module_sr)\n",
    "    data_module_sr.setup('test')\n",
    "    second_dataloader = data_module_sr.test_dataloader()\n",
    "    \n",
    "    def get_lpips_mean(dataloader1,dataloader2,lpips_model,device,dataset_type):\n",
    "        lpips_model.to(device)\n",
    "        lpips_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} LPIPS on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                lpips = lpips_model(gt_images, sr_images, return_similarity=True)\n",
    "                lpips_list.append(lpips.cpu())\n",
    "        lpips_list = torch.cat(lpips_list).numpy()\n",
    "        lpips_mean = np.nanmean(lpips_list)\n",
    "        lpips_model.to('cpu')\n",
    "        return lpips_mean\n",
    "    \n",
    "    \n",
    "    \n",
    "    lpips_mean = get_lpips_mean(first_dataloader,second_dataloader,lpips_model,device,\"lpips\")\n",
    "    \n",
    "    print(\"lpips: \",lpips_mean)\n",
    "    \n",
    "    #### SSIM\n",
    "    \n",
    "    def ssim(img1, img2):\n",
    "        # Calculate SSIM (Structural Similarity Index)\n",
    "        ssim_val = torch.mean((2 * img1 * img2 + 1e-8) * (2 * torch.abs(img1 - img2) + 1e-8) / (img1**2 + img2**2 + 1e-8), dim=(1, 2, 3))\n",
    "        return ssim_val\n",
    "    \n",
    "    def get_ssim_mean(dataloader1,dataloader2,ssim,device,dataset_type):\n",
    "        ssim_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} SSIM on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                ssim_val = ssim(sr_images, gt_images)\n",
    "                ssim_list.append(ssim_val.cpu())\n",
    "        ssim_list = torch.cat(ssim_list).numpy()\n",
    "        ssim_mean = np.nanmean(ssim_list)\n",
    "        return ssim_mean\n",
    "    \n",
    "    ssim_mean = get_ssim_mean(first_dataloader,second_dataloader,ssim,device,\"ssim\")\n",
    "    print(\"ssim: \",ssim_mean)\n",
    "    \n",
    "    #### PSNR\n",
    "    \n",
    "    def psnr(img1, img2, max_val=1.0):\n",
    "        # Convert images to float tensors\n",
    "        img1 = img1.float()\n",
    "        img2 = img2.float()\n",
    "        \n",
    "        max_val = img1.max()\n",
    "        # Calculate MSE (Mean Squared Error)\n",
    "        mse = F.mse_loss(img1, img2)\n",
    "        \n",
    "        # Calculate PSNR (Peak Signal-to-Noise Ratio)\n",
    "        psnr = 20 * torch.log10(max_val / torch.sqrt(mse))\n",
    "        \n",
    "        return psnr.item()\n",
    "    \n",
    "    def get_psnr_mean(dataloader1,dataloader2,device,dataset_type):\n",
    "        psnr_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} PSNR on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                psnr_val = psnr(sr_images, gt_images)\n",
    "                psnr_list.append(psnr_val)\n",
    "        psnr_mean = np.nanmean(psnr_list)\n",
    "        return psnr_mean\n",
    "    \n",
    "    \n",
    "    psnr_mean = get_psnr_mean(first_dataloader,second_dataloader,device,\"psnr\")\n",
    "    print(\"psnr: \",psnr_mean)\n",
    "\n",
    "print_metrics_given_path(\"patchgan/\")\n",
    "print_metrics_given_path(\"patchgansed/\")\n",
    "print_metrics_given_path(\"pixelwise/\")\n",
    "print_metrics_given_path(\"pixelwise_sed/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges we have encountered when implementing the paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
