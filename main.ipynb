{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Project Version 2 Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Information and Our Information\n",
    "### **Paper Title:** SeD Semantic-Aware Discriminator for Image Super-Resolution\n",
    "### **Paper Authors:** Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen\n",
    "\n",
    "### **Authors:**  Yigit Ekin and Mustafa Utku Aydogdu\n",
    "### **Mail:** e270207@metu.edu.tr e270206@metu.edu.tr\n",
    "### **Paper Description:** \n",
    "### Github Repository: [Link](https://github.com/YigitEkin/sed)\n",
    "\n",
    "In this work, researchers highlight the use of Generative Adversarial Networks (GANs) for image super-resolution tasks, particularly focusing on texture recovery. They note a limitation in existing methods where a single discriminator is employed to teach the super-resolution network the distribution of high-quality real-world images, leading to coarse learning and unexpected output. To address this, they introduce a Semantic-aware Discriminator (SeD), which incorporates image semantics to guide the network in learning fine-grained image distributions.\n",
    "\n",
    "The SeD leverages image semantics extracted from a trained semantic model, allowing the discriminator to discern real and fake images based on different semantic conditions. By integrating semantic features into the discriminator using spatial cross-attention modules, they aim to enhance the SR network's ability to generate more realistic and visually appealing images. The approach capitalizes on pretrained vision models and extensive datasets to enrich the understanding of image semantics and improve the fidelity of super-resolved images.\n",
    "\n",
    "\n",
    "Authors suggest that Vanilla Discriminators ignore the important semantics of the inputs, hence giving  semantic features of an image ( extracted via a pretrained network ), enables a better discriminator and hence a better feedback for the generator. The situation is demonstrated better on Figure 1, giving semantic features as condition enables the discriminator to specialize by finding boundaries within classes.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/fig1.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "\n",
    "A classical setup of Super Resolution GAN Network is as below. The generator network takes the low resolution image as input and produces a high resolution image. The discriminator takes the generated and the ground truth high resolution images and classifies as real or fake. In our setup, our generator takes 64x64 low resolution images and generate 256x256 high resolution images.\n",
    "\n",
    "<img src=\"img/sr_gan_setup.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "The proposed setup of the paper is as below, now the semantic feature maps of the ground truth high resolution images is also given as input to the discriminator. \n",
    "\n",
    "\n",
    "<img src=\"img/sed_gan_setup.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "Authors employ two discriminator types, a patch-based discriminator and a pixel-wise discriminator. \n",
    "The proposed architecture of the Patch-wise Semantic Aware Discriminator is shown below. \n",
    "\n",
    "\n",
    "<img src=\"img/patchwise_sed.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Patchwise SED</summary>\n",
    "\n",
    "  ```python\n",
    "class DownSampler(nn.Module):\n",
    "    # Downsamples 4 times in a conv, bn, leaky relu fashion that halves the spatial dimensions in each step and doubles the number of filters\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters * 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_filters * 4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_filters * 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchDiscriminatorWithSeD(nn.Module):\n",
    "    # PatchGAN discriminator with semantic-aware fusion blocks \n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        #First downsample the input size from 256x256 to 16x16 to match the semantic feature map size\n",
    "        self.downsampler = DownSampler(input_channels, num_filters)\n",
    "        #Use 3 semantic-aware fusion blocks to fuse the semantic feature maps with the downsampled input\n",
    "        self.semantic_aware_fusion_block1 = SemanticAwareFusionBlock()\n",
    "        self.semantic_aware_fusion_block2 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        self.semantic_aware_fusion_block3 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        #Final convolution to get the output\n",
    "        self.final_conv = nn.Conv2d(num_filters * 16, 1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, semantic_feature_maps, fs):\n",
    "        x = self.downsampler(fs)\n",
    "        x = self.semantic_aware_fusion_block1(semantic_feature_maps, x)\n",
    "        x = self.semantic_aware_fusion_block2(semantic_feature_maps, x)\n",
    "        x = self.semantic_aware_fusion_block3(semantic_feature_maps, x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "\n",
    "The discriminator has a specialized block called Semantic Aware Fusion Block. Semantic Aware Fusion Block takes the ground truth semantic features extracted by CLIP Feature Extractor, and applies cross attention between either ground truth or generated high resolution images as shown below. First the generated (or ground truth) feature maps are processed through normalization and self-attention mechanism , and then cross attention is applied. \n",
    "\n",
    "\n",
    "<img src=\"img/semantic_aware_fb.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Semantic Aware Fusion Block</summary>\n",
    "\n",
    "  ```python\n",
    "class SemanticAwareFusionBlock(nn.Module):\n",
    "    def __init__(self, channel_size_changer_input_nc=512):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, 1024) \n",
    "\n",
    "        self.channel_size_changer1 = nn.Conv2d(in_channels=channel_size_changer_input_nc, out_channels=128, kernel_size=1)\n",
    "        self.reduce_channels2 = nn.Conv2d(in_channels=1024, out_channels=128, kernel_size=1)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(128)\n",
    "        self.layer_norm_2 = nn.LayerNorm(128)\n",
    "        self.layer_norm_3 = nn.LayerNorm(128)\n",
    "\n",
    "        self.self_attention = SelfAttention(128, num_heads=1, dimensionality=128)\n",
    "        self.cross_attention = CrossAttention(128, heads=1, dim_head=128)\n",
    "\n",
    "        self.GeLU = nn.GELU()\n",
    "\n",
    "        #define 1x1 convolutions\n",
    "        self.increase_channels1 = nn.Conv2d(256, 1024, 1)\n",
    "\n",
    "    def forward(self, semantic_feature_maps, fs):\n",
    "        # fs ( or sh for generated) have shape batch, 3 x 16 x 16\n",
    "        #semantic feature maps  have shape batch x 1024 x 16 x 16\n",
    "        final_permute_height = semantic_feature_maps.shape[2]\n",
    "        final_permute_width = semantic_feature_maps.shape[3]\n",
    "        \n",
    "        #first handle S_h\n",
    "        semantic_feature_maps = self.group_norm(semantic_feature_maps)\n",
    "\n",
    "        #reduce the channel dimensions for the feature maps from 1024 to 128 for computation\n",
    "        semantic_feature_maps = self.reduce_channels2(semantic_feature_maps)\n",
    "\n",
    "\n",
    "        # Permute dimensions to rearrange the tensor\n",
    "        semantic_feature_maps = semantic_feature_maps.permute(0, 2, 3, 1).contiguous().view(semantic_feature_maps.size(0), -1, semantic_feature_maps.size(1))\n",
    "\n",
    "        #apply layer normalization\n",
    "        semantic_feature_maps = self.layer_norm_1(semantic_feature_maps)\n",
    "\n",
    "        #apply self attention\n",
    "        semantic_feature_maps = self.self_attention(semantic_feature_maps) #returned has shape 1,196,128 for now\n",
    "        #apply layer normalization\n",
    "        query = self.layer_norm_2(semantic_feature_maps)\n",
    "\n",
    "        #now handle fs or  sh\n",
    "        #reduce the channel dimensions for the sh\n",
    "\n",
    "        #make number of channels = 128 to be compatible with the semantic feature maps\n",
    "        fs = self.channel_size_changer1(fs)\n",
    "\n",
    "        #to use fs as residual, obtain a clone, \n",
    "        #note that gradient still accumulates in the original fs, so no problem\n",
    "        fs_residual = fs.clone()\n",
    "\n",
    "        #permute the dimensions\n",
    "        fs = fs.permute(0, 2, 3, 1).contiguous().view(fs.size(0), -1, fs.size(1))\n",
    "\n",
    "        #apply cross attention, query is the semantic feature maps and fs is the key and value\n",
    "        out = self.cross_attention(query, fs)\n",
    "\n",
    "        #apply layer normalization\n",
    "        out = self.layer_norm_3(out)\n",
    "\n",
    "        #apply GeLU\n",
    "        out = self.GeLU(out)\n",
    "\n",
    "        #permute the dimensions\n",
    "        out = out.permute(0,2,1).contiguous().view(out.size(0), -1, final_permute_height, final_permute_width)\n",
    "\n",
    "        #add the residual\n",
    "        output = torch.cat((out,fs_residual), dim=1)\n",
    "\n",
    "        #increase the channels back to 1024\n",
    "        output = self.increase_channels1(output)\n",
    "    \n",
    "        return output\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "The architecture of the CLIP Feature Extractor is demonstrated below. The CLIP Feature Extractor has normally 4 layers, but authors suggest using the outputs of third layer as going further causes loss of spatial information which is problematic while restoring a high resolution image. The architecture is shown below.\n",
    "\n",
    "\n",
    "<img src=\"img/clip_feature_extractor.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>CLIP Feature Extractor</summary>\n",
    "\n",
    "  ```python\n",
    "\n",
    "class CLIPRN50(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  \n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "        #add the openai-provided normalization\n",
    "        #https://github.com/jianjieluo/OpenAI-CLIP-Feature/blob/01269a8fceb540d3b6477b43177ea33845c9514c/clip/clip.py#L82C9-L82C92\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "        #load\n",
    "        self.ckpt_path = \"RN50\"\n",
    "        self.load_ckpt(self.ckpt_path)\n",
    "        self.freeze()\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = self.preprocess(x)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "The pixel-wise discriminator has U-Net architecture, which also employs the Semantic-Aware Fusion Block. The authors use both patch-based and pixel-wise based discriminators to demonstrate effectiveness of the Semantic-Aware Fusion Block. The architecture of the Pixel-wise Semantic Aware Discriminator is shown below.  \n",
    "\n",
    "<img src=\"img/pixelwise_sed.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>Pixelwise SED</summary>\n",
    "\n",
    "  ```python\n",
    "\n",
    "class DownSamplerPx(nn.Module):\n",
    "    #downsamples 4 times in a conv, bn, leaky relu fashion that halves the spatial dimensions in each step and doubles the number of filters\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(num_filters, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_filters)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        return x\n",
    "    \n",
    "class UNetPixelDiscriminatorwithSed(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_filters=64):\n",
    "        super(UNetPixelDiscriminatorwithSed, self).__init__()\n",
    "\n",
    "        #downsampler takes 256x256 images and downsamples to the 16x16\n",
    "        #to make dimensionality compatible with semantic feature maps\n",
    "        self.downsampler = DownSamplerPx(in_channels, num_filters)\n",
    "        \n",
    "        # Semantic Aware Fusion Blocks\n",
    "        self.semantic_aware_fusion_block1 = SemanticAwareFusionBlock(channel_size_changer_input_nc=64)\n",
    "        self.semantic_aware_fusion_block2 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        self.semantic_aware_fusion_block3 = SemanticAwareFusionBlock(channel_size_changer_input_nc=1024)\n",
    "        \n",
    "        self.upconv1 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)\n",
    "        self.upconv2 = nn.Conv2d(1024, 64, kernel_size=1, stride=1)\n",
    "        self.upconv3 = nn.Conv2d(64, 3, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "    def forward(self,semantic_feature_maps, fs):\n",
    "        x = self.downsampler(fs)\n",
    "        enc1 = self.semantic_aware_fusion_block1(semantic_feature_maps, x)\n",
    "        enc2 = self.semantic_aware_fusion_block2(semantic_feature_maps, enc1)\n",
    "        enc3 = self.semantic_aware_fusion_block3(semantic_feature_maps, enc2)\n",
    "        \n",
    "        dec = self.upconv1(enc3 + enc2)\n",
    "        dec = self.upconv2(dec + enc1)\n",
    "        dec = self.upconv3(dec + x)\n",
    "        \n",
    "        return dec\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Throughout the experiments, we use the RRDB Generator proposed in ESRGAN paper, whose building blocks are shown in below Figure.\n",
    "\n",
    "<img src=\"img/rrdb_generator.png\" style=\"width:700px; height:auto; display: flex; justify-content: center\"/> <br/> <br/>\n",
    "\n",
    "<details>\n",
    "  <summary>RRDB Generator</summary>\n",
    "\n",
    "  ```python\n",
    "class DenseBlock(nn.Module):\n",
    "  '''\n",
    "  Dense Block structure from https://arxiv.org/pdf/1809.00219 Fig4 : Left\n",
    "  '''\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=5, is_upsample=False):\n",
    "        super().__init__()\n",
    "        self.blocks = make_blocks(in_channels, out_channels, num_blocks, is_upsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prev_features = x\n",
    "        for block in self.blocks:\n",
    "            current_output = block(prev_features)\n",
    "            prev_features = torch.cat([prev_features, current_output], dim=1)\n",
    "        return x + current_output * 0.2\n",
    "\n",
    "class Residual_in_ResidualBlock(nn.Module):\n",
    "  '''\n",
    "  RRDB  structure from https://arxiv.org/pdf/1809.00219 Fig4 : Right\n",
    "  consists of 3 Dense Blocks\n",
    "  '''\n",
    "    def __init__(self, in_channels, num_blocks=3, is_upsample=False):\n",
    "        super().__init__()\n",
    "        self.rrdb1 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        self.rrdb2 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        self.rrdb3 = DenseBlock(in_channels, in_channels, num_blocks, is_upsample)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.rrdb1(x)\n",
    "        out2 = self.rrdb2(out1)\n",
    "        out3 = self.rrdb3(out2)\n",
    "        return x + out3 * 0.2\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    '''ESRGAN Generator, which consists of 23 Residual in Residual Dense Blocks\n",
    "    paper : https://arxiv.org/pdf/1809.00219\n",
    "    '''\n",
    "    def __init__(self, in_channels=3, num_channels=64, num_blocks=23, clip_output=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_layer(in_channels, num_channels)\n",
    "        self.conv2 = get_layer(num_channels, num_channels)\n",
    "        self.conv3 = get_layer(num_channels, num_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.output = get_layer(num_channels, in_channels)\n",
    "        self.first_ups = get_layer(num_channels, num_channels, is_upsample=True)\n",
    "        self.second_ups = get_layer(num_channels, num_channels, is_upsample=True)\n",
    "        self.rrdb = nn.Sequential(*[Residual_in_ResidualBlock(num_channels) for _ in range(num_blocks)])\n",
    "        self.clip_output = clip_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv1(x)\n",
    "        x = self.rrdb(res)\n",
    "        x = self.conv2(x)\n",
    "        x = x + res\n",
    "        x = self.first_ups(x)\n",
    "        x = self.second_ups(x)\n",
    "        x = self.act(self.conv3(x))\n",
    "        if self.clip_output:\n",
    "            x = self.output(x).clip(-1, 1)\n",
    "        else:\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "         \n",
    "To see the effect of Semantic Aware Fusion Block , we also implemented Vanilla Patch-wise Discriminator and Vanilla Pixel-wise Discriminator.\n",
    "\n",
    "<details>\n",
    "  <summary>Patch-wise Discriminator</summary>\n",
    "\n",
    "  ```python\n",
    "#Vanilla patchgan discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters=64):\n",
    "        super().__init__()\n",
    "        #Downsample the input size from 256x256 to 16x16\n",
    "        self.downsampler = DownSampler(input_channels, num_filters)\n",
    "        self.final_conv = nn.Conv2d(num_filters * 8, 1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, fs):\n",
    "        fs = self.downsampler(fs)\n",
    "        fs = self.final_conv(fs)\n",
    "        return fs\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Pixel-wise Discriminator </summary>\n",
    "\n",
    "  ```python\n",
    "class UNetPixelDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_filters=64):\n",
    "        super(UNetPixelDiscriminator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            self._conv_block(in_channels, num_filters),\n",
    "            self._conv_block(num_filters, num_filters),\n",
    "            self._conv_block(num_filters, num_filters * 2),\n",
    "            self._conv_block(num_filters * 2, num_filters * 4),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(num_filters * 4, num_filters * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            self._upconv_block(num_filters * 4, num_filters * 4),\n",
    "            self._upconv_block(num_filters * 4, num_filters * 2),\n",
    "            self._upconv_block(num_filters * 2, num_filters),\n",
    "            self._upconv_block(num_filters, num_filters),\n",
    "            nn.Conv2d(num_filters, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def _upconv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, fs):\n",
    "        # Encoder\n",
    "        # fs = 64\n",
    "        enc1 = self.encoder[0](fs) # 32x32x64\n",
    "        enc2 = self.encoder[1](enc1) # 16x16x64\n",
    "        enc3 = self.encoder[2](enc2) # 8x8x128\n",
    "        enc4 = self.encoder[3](enc3) # 4x4x256\n",
    "\n",
    "        #Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4) # 2x2x256\n",
    "\n",
    "        #Decoder with skip connections using addition\n",
    "        dec = self.decoder[0](bottleneck) # 4x4x256\n",
    "        dec = self.decoder[1](dec + enc4) # 8x8x128\n",
    "        dec = self.decoder[2](dec + enc3) # 16x16x64\n",
    "        dec = self.decoder[3](dec + enc2) # 32x32x64\n",
    "        dec = self.decoder[4](dec + enc1) # 64x64x1\n",
    "\n",
    "        return dec\n",
    "```\n",
    "</details>\n",
    "\n",
    "### **Our Assumptions:**\n",
    "Implementing a super resolution model based solely on a paper, without access to the accompanying code, was challenging due to the complexities of understanding and implementing the loss function, architecture, and performance metrics described in the paper. Dealing with dimensionality inconsistencies in paper. Some are listed below.\n",
    "\n",
    "* we assumed that the group normalization has 32 groups (not stated in the paper)\n",
    "* we assumed that the conv block in patchwise discriminator is a  convolution block that doubles the channel size and with kernel_size of 4, stride=2 and padding=1 followed by a batch normalization block followed by a leaky relu block (not included in the last convolution block) which is not stated in the paper.\n",
    "* They did not specified the adverserial loss function details. As a result, we have decided to go with wassertein loss with gradient penalty to achieve a more stable training.\n",
    "* They did not specify how they have preprocessed the dataset. As a result, due to small number of images in the dataset, we have decided to conduct a literature survey on how different models have overcome this issue and found that ESRGAN does combine 2 datasets and crops random patches from each image to increase the number of images.\n",
    "* We have decided to move with crop size of 400 for hr images and 100 for lr images. This means that during training our model inputs 100x100 crops and tries to generate 400x400 hr version of it.\n",
    "* For cross attention, we have decided to use single head attention rather than multi head attention\n",
    "* CLIP preprocessor normally downscales the image to 224x224 before extracting embeddings. We believed that this can downgrade the performance w.r.t hr images as a result, we did not use this preprocessor.\n",
    "* To obtain same spatial dimensionality with the clip embeddings (for concatenation specified in the image below in part d), we added extra convolution layer that did not change the channel size but decreases the spatial dimensions.\n",
    "* The authors did not describe the weight (lambda) values of the loss functions as a result, we have decided to go with 1 for mse and 10 for gradient penalty in wasserstein loss\n",
    "* The authors did not specify whether they have used multi-head attention or single head attention. As a result, we have decided to go with single head attention because we thought it should be sufficient enough.\n",
    "* The authors did not specify the dimensionlity of attention head. So, we have decided to go with 128 as this will result in 8 times less memory usage. \n",
    "* For the coefficients of the losses (i.e VGG, adverserial, MSE), we have conducted several experiments and the current setup in the config files are the ones that have achieved the best scores. One thing that we have tried to keep constant is the ratio between the losses. For example, if the VGG loss is 0.1 times the adverserial loss, we have tried to keep this ratio constant in all experiments by changing the coefficients.\n",
    "* The authors did not specify how they have calculated the psnr, ssim, lpips scores for the datasets. Given that they have given qualitative results as crops from image, we have calculated the scores by using random crops as ground truth images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters of your model\n",
    "\n",
    "We aim to compare the effect of SeD discriminator with vanilla discriminator. As a result, we have two different training setups. Before reading the hyperparameters, please note that the hyperparameters are the same for both models except for the discriminator part. In addition, the losses used for the model can be seen from the image below where L_s is VGG  perceptual loss, L_p is the pixelwise MSE loss and L_adv is the adverserial loss.\n",
    "\n",
    "\n",
    "<img src=\"img/losses.png\"> <br/> <br/>\n",
    "The hyperparameters of the models are as follows:\n",
    "\n",
    "### Vanilla Patchgan Discriminator\n",
    "- **Train Batch Size**: 16\n",
    "- **Image Size**: 256\n",
    "- **Downsample Factor**: 4 (downsampling factor for low-resolution images)\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 1000.0\n",
    "    - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1e-1\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5\n",
    "\n",
    "### Patch SeD\n",
    "- **Train Batch Size**: 16\n",
    "- **Image Size**: 256\n",
    "- **Downsample Factor**: 4 (downsampling factor for low-resolution images)\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 1000.0\n",
    "    - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1e-1\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5\n",
    "\n",
    "\n",
    "### Vanilla Pixelwise Discriminator\n",
    "- **Train Batch Size**: 16\n",
    "- **Image Size**: 256\n",
    "- **Downsample Factor**: 4 (downsampling factor for low-resolution images)\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 1000.0\n",
    "    - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1e-1\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5\n",
    "\n",
    "### Pixelwise SeD\n",
    "- **Train Batch Size**: 16\n",
    "- **Image Size**: 256\n",
    "- **Downsample Factor**: 4 (downsampling factor for low-resolution images)\n",
    "- **Losses**:\n",
    "  - **VGG**:\n",
    "    - `weight`: 1000.0\n",
    "    - `output_layer_idx`: 23 (index of the layer to extract features from)\n",
    "  - **Adversarial_G**:\n",
    "    - `weight`: 1.0\n",
    "  - **MSE**:\n",
    "    - `weight`: 1e-1\n",
    "  - **Adversarial_D**:\n",
    "    - `r1_gamma`: 10.0 (constant for wasserstein GP)\n",
    "    - `r2_gamma`: 0.0 (constant for wasserstein GP)\n",
    "- **Super Resolution Module Configuration**:\n",
    "  - `generator_learning_rate`: 1e-4\n",
    "  - `discriminator_learning_rate`: 1e-5\n",
    "  - `generator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `discriminator_decay_steps`: [50_000, 100_000, 150_000, 200_000, 250_000]\n",
    "  - `generator_decay_gamma`: 0.5\n",
    "  - `discriminator_decay_gamma`: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with SeD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPORTANT NOTE:** the training of the model is done on a remote server where we have not used jupyter notebook. Normally, scripts in the cells below are used to train the model. However, in order to not overly crowd the jupyter notebook for the reviewers, we have included the code that is responsible for training but the training logs will be displayed in the last cell of this section named as training loop which abstracts all this logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING CONFIG\n",
    "\n",
    "### Note for Device Choice : To select device, change line 8 on any config file:\n",
    "Line 8 contains the following:\n",
    " ```python\n",
    " device = torch.device(\"cuda\") if accelerator==\"gpu\" else torch.device(\"cpu\")\n",
    " ```\n",
    "Change device to `cpu`, `cuda` or do not change if you want to select the best available option\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "###########################\n",
    "##### Dataset Configs #####\n",
    "###########################\n",
    "\n",
    "dataset_module = dict(\n",
    "    num_workers=4,\n",
    "    train_batch_size=train_batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    train_dataset_config=dict(image_size=256, image_dir_hr=\"data/dataset_cropped/hr\", image_dir_lr=\"data/dataset_cropped/lr\", downsample_factor=4,mirror_augment_prob=0.5),\n",
    "    val_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    "    test_dataset_config=dict(image_size=256, image_dir_hr=\"data/evaluation/hr/manga109\", image_dir_lr=\"data/evaluation/lr/manga109\"),\n",
    ")\n",
    "\n",
    "##################\n",
    "##### Losses #####\n",
    "##################\n",
    "vgg_ckpt_path=\"pretrained_models/vgg16.pth\"\n",
    "loss_dict = dict(\n",
    "    VGG=dict(weight=5e-5, model_config=dict(path=vgg_ckpt_path, output_layer_idx=23, resize_input=False)),\n",
    "    Adversarial_G=dict(weight=1.0),\n",
    "    MSE=dict(weight=1.0),\n",
    "    Adversarial_D=dict(r1_gamma=10.0, r2_gamma=0.0)\n",
    ")\n",
    "\n",
    "#########################\n",
    "##### Model Configs #####\n",
    "#########################\n",
    "\n",
    "super_resolution_module_config = dict(loss_dict=loss_dict, \n",
    "    generator_learning_rate=1e-4, discriminator_learning_rate=1e-5, \n",
    "    generator_decay_steps=[50_000, 100_000, 150_000, 200_000, 250_000], \n",
    "    discriminator_decay_steps=[50_000, 100_000, 150_000, 200_000, 250_000], \n",
    "    generator_decay_gamma=0.5, discriminator_decay_gamma=0.5,\n",
    "    clip_generator_outputs=False,\n",
    "    use_sed_discriminator=True)\n",
    "\n",
    "#######################\n",
    "###### Callbacks ######\n",
    "#######################\n",
    "\n",
    "ckpt_callback = dict(every_n_train_steps=4000, save_top_k=1, save_last=True, monitor='fid_test', mode='min')\n",
    "synthesize_callback_train = dict(num_samples=12, eval_every=2000) # TODO: 4000\n",
    "synthesize_callback_test = dict(num_samples=6, eval_every=2000)\n",
    "fid_callback = dict(eval_every=4000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the following Models:\n",
    "- Patch-wise Discriminator with SeD\n",
    "- Vanilla Patch-wise Discriminator\n",
    "- Pixel-wise Discriminator with SeD\n",
    "- Vanilla Pixel-wise Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | generator     | RRDBNet                   | 15.4 M\n",
      "1 | discriminator | PatchDiscriminatorWithSeD | 4.7 M \n",
      "2 | clip          | CLIPRN50                  | 23.4 M\n",
      "------------------------------------------------------------\n",
      "20.1 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "43.5 M    Total params\n",
      "173.867   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:39,  3.28s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:15,  1.41s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:08,  1.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:04,  1.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:02,  3.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  4.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:04<00:00,  6.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  6.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  7.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.85it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.531, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.02it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.07it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.15it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:20<00:00,  1.30it/s, v_num=0, lpips_test=0.241, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  8.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.215, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.57it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.203, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.86it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.20it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.41it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.194, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.01it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.184, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.89it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.55it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.179, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.80it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.30it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.25it/s, v_num=0, lpips_test=0.174, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:21<00:00,  1.29it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.22it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:20<00:00,  1.29it/s, v_num=0, lpips_test=0.170, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:12,  1.00s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  2.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.55it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.85it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:21<00:00,  1.28it/s, v_num=0, lpips_test=0.164, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.34it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.38it/s]\u001b[A\n",
      "Epoch 100:  29%|▎| 8/28 [00:06<00:16,  1.20it/s, v_num=0, lpips_test=0.163, VGG_^C\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/patchgan_sed.py\" #Training of patchgan discriminator with SeD\n",
    "\n",
    "!python train.py --config_file=$CFG --device=device  # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | generator     | RRDBNet            | 15.4 M\n",
      "1 | discriminator | PatchDiscriminator | 2.8 M \n",
      "2 | clip          | CLIPRN50           | 23.4 M\n",
      "-----------------------------------------------------\n",
      "18.2 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.180   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:42,  3.55s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:16,  1.52s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:08,  1.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:04<00:01,  3.50it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:04<00:01,  4.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:04<00:00,  4.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  5.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  6.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.70it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.539, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  7.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.77it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.252, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.24it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.220, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:13,  1.12s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.46it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.205, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.35it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.194, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.41it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.89it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.21it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.182, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  2.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.40it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.60it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.50it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.90it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.176, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.84it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.59it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  7.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  7.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  8.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.67it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.30it/s, v_num=0, lpips_test=0.169, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.169, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  6.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  7.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.64it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.165, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.162, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.29it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.51it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.65it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.03it/s]\u001b[A\n",
      "Epoch 102:  64%|▋| 18/28 [00:13<00:07,  1.34it/s, v_num=0, lpips_test=0.159, VGG^C\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Epoch 102:  64%|▋| 18/28 [00:13<00:07,  1.33it/s, v_num=0, lpips_test=0.159, VGG\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/patchgan.py\" #Training of patchgan discriminator without SeD\n",
    "\n",
    "!python train.py --config_file=$CFG --device=device #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                          | Params\n",
      "----------------------------------------------------------------\n",
      "0 | generator     | RRDBNet                       | 15.4 M\n",
      "1 | discriminator | UNetPixelDiscriminatorwithSed | 3.2 M \n",
      "2 | clip          | CLIPRN50                      | 23.4 M\n",
      "----------------------------------------------------------------\n",
      "18.6 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "42.0 M    Total params\n",
      "167.802   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:36,  3.01s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:14,  1.31s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:03<00:07,  1.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:04,  2.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:02,  2.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:01,  3.72it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  4.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:03<00:00,  5.63it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:03<00:00,  6.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:03<00:00,  7.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  7.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  3.04it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.578, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.93it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  7.49it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  8.59it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.09it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.243, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.19it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.31it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:19<00:00,  1.35it/s, v_num=0, lpips_test=0.215, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.86it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.51it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  8.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.43it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.77it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.202, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.37it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  8.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.87it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.78it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.192, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:14,  1.22s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:06,  1.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  3.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:02<00:02,  2.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:02<00:01,  3.79it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:02<00:01,  4.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:02<00:00,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  4.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  4.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  5.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:03<00:00,  4.14it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.185, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.90it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.66it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.180, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.99it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.64it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.47it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:09<00:12,  1.32it/s, v_num=0, lpips_test=0.174, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.58it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:19<00:00,  1.36it/s, v_num=0, lpips_test=0.171, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:16,  1.37s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:06,  1.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.60it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  3.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:02<00:02,  2.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:02<00:01,  3.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:02<00:01,  4.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:02<00:00,  5.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  5.00it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  5.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  6.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:03<00:00,  4.10it/s]\u001b[A\n",
      "Epoch 99:  96%|▉| 27/28 [00:19<00:00,  1.37it/s, v_num=0, lpips_test=0.164, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.55it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.69it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.67it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.45it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.56it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.09it/s]\u001b[A\n",
      "Epoch 107:   0%| | 0/28 [00:00<?, ?it/s, v_num=0, lpips_test=0.160, VGG_step=6.4^C\n"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/pixelwise_sed.py\" #Training of pixelwise discriminator with SeD\n",
    "\n",
    "!python train.py --config_file=$CFG --device=device #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | generator     | RRDBNet                | 15.4 M\n",
      "1 | discriminator | UNetPixelDiscriminator | 3.5 M \n",
      "2 | clip          | CLIPRN50               | 23.4 M\n",
      "---------------------------------------------------------\n",
      "19.0 M    Trainable params\n",
      "23.4 M    Non-trainable params\n",
      "42.3 M    Total params\n",
      "169.295   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0:   0%|                                           | 0/28 [00:00<?, ?it/s]\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:03<00:37,  3.10s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:03<00:14,  1.34s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:03<00:05,  1.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:03<00:03,  2.38it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:03<00:02,  3.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:03<00:01,  3.92it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:03<00:00,  5.47it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:04<00:00,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:04<00:00,  6.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  92%|█████▌| 12/13 [00:04<00:00,  7.48it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:04<00:00,  2.98it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.532, VGG_s\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.74it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.95it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.03it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.96it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.31it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.30it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 27/28 [00:22<00:00,  1.20it/s, v_num=0, lpips_test=0.238, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.25it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.57it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.81it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.82it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.14it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.35it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.216, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.22it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.44it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.63it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.73it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.70it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.11it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.58it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.24it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 39:  96%|▉| 27/28 [00:22<00:00,  1.21it/s, v_num=0, lpips_test=0.201, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:13,  1.11s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  3.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  4.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  5.78it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  6.68it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.46it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:02<00:00,  7.62it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  8.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  8.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.38it/s]\u001b[A\n",
      "Epoch 49:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.189, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.66it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  3.98it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.88it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.77it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.55it/s]\u001b[A\n",
      "Epoch 59:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.182, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:04,  2.21it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.42it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  4.61it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.71it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.54it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.01it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.52it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  8.91it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.95it/s]\u001b[A\n",
      "Epoch 69:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.174, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:08,  1.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:03,  2.75it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:00<00:02,  4.10it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.34it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.39it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.28it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.97it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.08it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.27it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:01<00:00,  6.58it/s]\u001b[A\n",
      "Epoch 71:  43%|▍| 12/28 [00:10<00:13,  1.17it/s, v_num=0, lpips_test=0.167, VGG_/kuacc/users/hpc-yekin/.conda/envs/sed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='fid_test')` could not find the monitored key in the returned metrics: ['lpips_test', 'VGG', 'VGG_step', 'Adversarial_G', 'Adversarial_G_step', 'MSE', 'MSE_step', 'Adversarial_D', 'Adversarial_D_step', 'VGG_epoch', 'Adversarial_G_epoch', 'MSE_epoch', 'Adversarial_D_epoch', 'epoch', 'step']. HINT: Did you call `log('fid_test', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 79:  96%|▉| 27/28 [00:22<00:00,  1.21it/s, v_num=0, lpips_test=0.167, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:01<00:14,  1.17s/it]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:01<00:05,  1.85it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:03,  2.94it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:02,  4.07it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  5.16it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:01,  6.17it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  7.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.33it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:02<00:00,  8.76it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:02<00:00,  9.09it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  5.44it/s]\u001b[A\n",
      "Epoch 89:  96%|▉| 27/28 [00:22<00:00,  1.22it/s, v_num=0, lpips_test=0.166, VGG_\n",
      "Calculating test LPIPS on sr images:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:   8%|▌      | 1/13 [00:00<00:09,  1.23it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  15%|█      | 2/13 [00:00<00:04,  2.53it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  23%|█▌     | 3/13 [00:01<00:02,  3.83it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  31%|██▏    | 4/13 [00:01<00:01,  5.06it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  38%|██▋    | 5/13 [00:01<00:01,  6.13it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  46%|███▏   | 6/13 [00:01<00:00,  7.04it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  54%|███▊   | 7/13 [00:01<00:00,  7.77it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  62%|████▎  | 8/13 [00:01<00:00,  8.36it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  69%|████▊  | 9/13 [00:01<00:00,  8.80it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  77%|████▌ | 10/13 [00:01<00:00,  9.12it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images:  85%|█████ | 11/13 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Calculating test LPIPS on sr images: 100%|██████| 13/13 [00:02<00:00,  6.39it/s]\u001b[A\n",
      "Epoch 99:  39%|▍| 11/28 [00:09<00:14,  1.17it/s, v_num=0, lpips_test=0.161, VGG_"
     ]
    }
   ],
   "source": [
    "CFG=\"configs/pixelwise.py\" #Training of pixelwise discriminator without SeD\n",
    "\n",
    "!python train.py --config_file=$CFG --device=device #--debug # --resume_from logs/sed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>\n",
    "        <div>\n",
    "            <h3>Training Details:</h3>\n",
    "            The authors state in the paper that they have conducted a training of 35 hours with 4 tesla V100 GPU's with a batch size of 8. We, due to resource constraints, were not eligible to 4 Tesla V100 GPU's for distributed training. As a result, we have conducted a training with a single Nvidia A40 with a batch size of 16 for approximately 45 hours. <br/>\n",
    "            The results given below are the training loss curves and lpips test logs of the original training we have conducted. The training logs are not included in the notebook due to the large size of the logs. Hence, we have only provided the screenshot of the logs.\n",
    "        </div>\n",
    "  </summary>\n",
    "\n",
    "  <details>\n",
    "      <summary><h4>Patchgan SeD:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_d_ptch_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_ptc_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_ptch_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_ptch_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_test_ptch_sed.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "\n",
    "  <details>\n",
    "      <summary><h4>Vanilla Patchgan:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_d_ptc.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_ptch.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_ptc.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_ptch.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_ptc.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "  <details>\n",
    "      <summary><h4>Pixelwise SeD:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_d_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_l_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_px_sed.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_px_sed.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "  <details>\n",
    "      <summary><h4>Vanilla Pixelwise Discriminator:</h4></summary>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center;\">\n",
    "      <img src=\"img/adv_d_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/adv_g_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/mse_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      <img src=\"img/vgg_px.png\" style=\"width: 600px; height:auto;\"/>\n",
    "      </div>\n",
    "      <div style=\"display:flex; justify-content:center; align-items:center; margin-top:50px;\">\n",
    "      <img src=\"img/lpips_px.png\" style=\"width: 800px; height:auto;\"/>\n",
    "      </div>\n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "**Important Note:** currently this script generates super-resolved results for the Set5 dataset. If you want to generate results for other datasets, you need to change the paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FID on SR images: 100%|██████████| 5/5 [00:01<00:00,  4.24it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 5/5 [00:01<00:00,  4.07it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 5/5 [00:00<00:00,  5.30it/s]\n",
      "Calculating FID on SR images: 100%|██████████| 5/5 [00:01<00:00,  4.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from models.super_resolution_module import SuperResolutionModule\n",
    "from datasets.dataset_module import DatasetModule\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def postprocess_image(image, min_val=-1.0, max_val=1.0):\n",
    "    image = image.astype(np.float64)\n",
    "    image = np.clip(image, -1, 1)\n",
    "    image = (image - min_val) * 255 / (max_val - min_val)\n",
    "    image = image.astype(np.uint8)\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    return image\n",
    "\n",
    "def generate_results(ckpt, image_dir_hr, image_dir_lr, save_path, device):\n",
    "    model = SuperResolutionModule.load_from_checkpoint(ckpt) \n",
    "    train_batch_size = 1  # given so that each image is processed by itself\n",
    "    val_batch_size = 1 # given so that each image is processed by itself\n",
    "    test_batch_size = 1 # given so that each image is processed by itself\n",
    "\n",
    "    model.eval()\n",
    "    dataset_module = dict(\n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr, downsample_factor=4),\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr),\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=image_dir_hr, image_dir_lr=image_dir_lr),\n",
    "    )\n",
    "\n",
    "\n",
    "    data_module_gt = DatasetModule(**dataset_module)\n",
    "    data_module_gt.setup('test')\n",
    "    dataloader = data_module_gt.test_dataloader()\n",
    "\n",
    "    os.makedirs(\"results_Set5\", exist_ok=True)\n",
    "    os.makedirs(f\"results_Set5/{save_path}\", exist_ok=True)\n",
    "\n",
    "    os.makedirs(\"gt_Set5\", exist_ok=True)\n",
    "    \n",
    "    cnt = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Generating Results on SR images\", total=len(dataloader)):\n",
    "        sr_images = model.make_high_resolution(batch)\n",
    "        sr = sr_images['generated_super_resolution_image'].to(device)\n",
    "        hr = batch['image_hr'].to(device)\n",
    "        #save the sr images to the \"sr_pngs\" folder\n",
    "        for i in range(len(sr)):\n",
    "            img = sr[i]\n",
    "            hr_img = hr[i]\n",
    "            \n",
    "            img = postprocess_image(img.detach().cpu().numpy())\n",
    "            img = Image.fromarray(img)\n",
    "            img.save(f\"results_Set5/{save_path}/{cnt}.png\")\n",
    "            \n",
    "            hr_img = postprocess_image(hr_img.detach().cpu().numpy())\n",
    "            hr_img = Image.fromarray(hr_img)\n",
    "            hr_img.save(f\"gt_Set5/{cnt}.png\")\n",
    "\n",
    "\n",
    "            \n",
    "            cnt += 1\n",
    "    \n",
    "torch.manual_seed(1256)\n",
    "np.random.seed(1256)\n",
    "ckpt=\"model_weights/patchgan_sed.ckpt\"\n",
    "ckpt2=\"model_weights/pixelwise_sed.ckpt\"\n",
    "ckpt3=\"model_weights/pixelwise.ckpt\"\n",
    "ckpt4=\"model_weights/patchgan.ckpt\"\n",
    "\n",
    "generate_results_dict = {\n",
    "    \"patchgan\": ckpt4,\n",
    "    \"patchgan_sed\": ckpt,\n",
    "    \"pixelwise\": ckpt3,\n",
    "    \"pixelwise_sed\": ckpt2,\n",
    "} \n",
    "\n",
    "image_path_hr = \"data/evaluation/hr/Set5\"\n",
    "image_path_lr = \"data/evaluation/hr/Set5\"\n",
    "\n",
    "for key, item in generate_results_dict.items():\n",
    "    generate_results(item, image_path_hr, image_path_lr, key, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating metrics for tmp/results_Set5/patchgan/and tmp/gt_Set5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 5/5 [00:00<00:00, 25.54it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 5/5 [00:00<00:00, 11.68it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 5/5 [00:00<00:00, 25.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set5 using patchgan scores are:\n",
      " psnr: 28.32532958984375\n",
      " lpips: 0.07299451529979706 \n",
      " ssim: 0.8197302284707041\n",
      "calculating metrics for tmp/results_Set5/patchgan_sed/and tmp/gt_Set5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 5/5 [00:00<00:00, 23.60it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 5/5 [00:00<00:00, 11.65it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 5/5 [00:00<00:00, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set5 using patchgan_sed scores are:\n",
      " psnr: 28.52031478881836\n",
      " lpips: 0.07668115198612213 \n",
      " ssim: 0.8267332764951745\n",
      "calculating metrics for tmp/results_Set5/pixelwise/and tmp/gt_Set5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 5/5 [00:00<00:00, 47.34it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 5/5 [00:00<00:00, 15.25it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 5/5 [00:00<00:00, 26.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set5 using pixelwise scores are:\n",
      " psnr: 28.85939750671387\n",
      " lpips: 0.11080751568078995 \n",
      " ssim: 0.8417996802882358\n",
      "calculating metrics for tmp/results_Set5/pixelwise_sed/and tmp/gt_Set5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 5/5 [00:00<00:00, 23.49it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 5/5 [00:00<00:00, 15.21it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 5/5 [00:00<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set5 using pixelwise_sed scores are:\n",
      " psnr: 28.582693099975586\n",
      " lpips: 0.08782997727394104 \n",
      " ssim: 0.8272810757698196\n",
      "calculating metrics for tmp/results_Set14/patchgan/and tmp/gt_Set14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 14/14 [00:00<00:00, 65.74it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 14/14 [00:00<00:00, 16.67it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 14/14 [00:00<00:00, 60.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set14 using patchgan scores are:\n",
      " psnr: 25.425380979265487\n",
      " lpips: 0.12650790810585022 \n",
      " ssim: 0.7052855982882288\n",
      "calculating metrics for tmp/results_Set14/patchgan_sed/and tmp/gt_Set14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 14/14 [00:00<00:00, 56.17it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 14/14 [00:00<00:00, 15.91it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 14/14 [00:00<00:00, 98.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set14 using patchgan_sed scores are:\n",
      " psnr: 25.516498974391393\n",
      " lpips: 0.12707917392253876 \n",
      " ssim: 0.7115346855304715\n",
      "calculating metrics for tmp/results_Set14/pixelwise/and tmp/gt_Set14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 14/14 [00:00<00:00, 55.88it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 14/14 [00:00<00:00, 18.15it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 14/14 [00:00<00:00, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set14 using pixelwise scores are:\n",
      " psnr: 25.908776010785783\n",
      " lpips: 0.18113267421722412 \n",
      " ssim: 0.7328405892092797\n",
      "calculating metrics for tmp/results_Set14/pixelwise_sed/and tmp/gt_Set14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 14/14 [00:00<00:00, 73.07it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 14/14 [00:00<00:00, 17.03it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 14/14 [00:00<00:00, 65.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the Set14 using pixelwise_sed scores are:\n",
      " psnr: 25.551700592041016\n",
      " lpips: 0.14808118343353271 \n",
      " ssim: 0.7168674343844766\n",
      "calculating metrics for tmp/results_urban100/patchgan/and tmp/gt_urban100/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 128.13it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 16.76it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 159.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the urban100 using patchgan scores are:\n",
      " psnr: 23.356260175704957\n",
      " lpips: 0.13328886032104492 \n",
      " ssim: 0.7040493599656144\n",
      "calculating metrics for tmp/results_urban100/patchgan_sed/and tmp/gt_urban100/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 129.10it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:06<00:00, 16.65it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 136.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the urban100 using patchgan_sed scores are:\n",
      " psnr: 23.39994520187378\n",
      " lpips: 0.13472136855125427 \n",
      " ssim: 0.7065629538047355\n",
      "calculating metrics for tmp/results_urban100/pixelwise/and tmp/gt_urban100/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 129.07it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 18.57it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 138.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the urban100 using pixelwise scores are:\n",
      " psnr: 23.566014013290406\n",
      " lpips: 0.17821992933750153 \n",
      " ssim: 0.7208096088247611\n",
      "calculating metrics for tmp/results_urban100/pixelwise_sed/and tmp/gt_urban100/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 134.52it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 17.01it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 143.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the urban100 using pixelwise_sed scores are:\n",
      " psnr: 23.432228927612304\n",
      " lpips: 0.1529310643672943 \n",
      " ssim: 0.710573309456417\n",
      "calculating metrics for tmp/results_manga/patchgan/and tmp/gt_manga/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 109/109 [00:00<00:00, 157.99it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 109/109 [00:05<00:00, 18.69it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 109/109 [00:00<00:00, 174.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the manga109 using patchgan scores are:\n",
      " psnr: 26.605389043825483\n",
      " lpips: 0.05496285855770111 \n",
      " ssim: 0.824411791092232\n",
      "calculating metrics for tmp/results_manga/patchgan_sed/and tmp/gt_manga/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 109/109 [00:00<00:00, 138.84it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 109/109 [00:05<00:00, 18.74it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 109/109 [00:00<00:00, 145.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the manga109 using patchgan_sed scores are:\n",
      " psnr: 26.53113312677506\n",
      " lpips: 0.05527523159980774 \n",
      " ssim: 0.8234676195979304\n",
      "calculating metrics for tmp/results_manga/pixelwise/and tmp/gt_manga/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 109/109 [00:00<00:00, 136.92it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 109/109 [00:06<00:00, 17.66it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 109/109 [00:00<00:00, 162.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the manga109 using pixelwise scores are:\n",
      " psnr: 26.92198289643734\n",
      " lpips: 0.06947644054889679 \n",
      " ssim: 0.8361318013636717\n",
      "calculating metrics for tmp/results_manga/pixelwise_sed/and tmp/gt_manga/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 109/109 [00:00<00:00, 131.03it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 109/109 [00:06<00:00, 17.48it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 109/109 [00:00<00:00, 155.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the manga109 using pixelwise_sed scores are:\n",
      " psnr: 26.71990483835203\n",
      " lpips: 0.061318933963775635 \n",
      " ssim: 0.8284960323340378\n",
      "calculating metrics for tmp/results_DIV2K_valid_HR/patchgan/and tmp/gt_div2k/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 128.28it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 17.89it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 155.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the DIV2K using patchgan scores are:\n",
      " psnr: 31.41322151184082\n",
      " lpips: 0.08203309774398804 \n",
      " ssim: 0.831089736958795\n",
      "calculating metrics for tmp/results_DIV2K_valid_HR/patchgan_sed/and tmp/gt_div2k/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 124.99it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 16.87it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 180.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the DIV2K using patchgan_sed scores are:\n",
      " psnr: 31.53876937866211\n",
      " lpips: 0.08428461849689484 \n",
      " ssim: 0.8307865551960572\n",
      "calculating metrics for tmp/results_DIV2K_valid_HR/pixelwise/and tmp/gt_div2k/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 126.74it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 17.31it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 160.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the DIV2K using pixelwise scores are:\n",
      " psnr: 31.448500556945802\n",
      " lpips: 0.12308312207460403 \n",
      " ssim: 0.8460997286660628\n",
      "calculating metrics for tmp/results_DIV2K_valid_HR/pixelwise_sed/and tmp/gt_div2k/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lpips LPIPS on sr images: 100%|██████████| 100/100 [00:00<00:00, 128.90it/s]\n",
      "Calculating ssim SSIM on sr images: 100%|██████████| 100/100 [00:05<00:00, 17.45it/s]\n",
      "Calculating psnr PSNR on sr images: 100%|██████████| 100/100 [00:00<00:00, 144.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the DIV2K using pixelwise_sed scores are:\n",
      " psnr: 31.655537166595458\n",
      " lpips: 0.1033484935760498 \n",
      " ssim: 0.8364254826235448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "StarGAN v2\n",
    "Copyright (c) 2020-present NAVER Corp.\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "4.0 International License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
    "\"\"\"\n",
    "\n",
    "#WE HAVE IMPLEMENTED THIS CODE BLOCK BY USING THE REFERENCE AT THE TOP AS A GUIDANCE\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets.dataset_module import DatasetModule\n",
    "from losses.lpips.lpips import LPIPS\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "def print_metrics_given_path(sr_path, gt_path, label, device):\n",
    "    print(\"calculating metrics for \" + sr_path + \"and \" +  gt_path)\n",
    "    train_batch_size = 1  # given as temporary data\n",
    "    val_batch_size = 1 # given as temporary data\n",
    "    test_batch_size = 1 # given as temporary data\n",
    "    \n",
    "    \n",
    "    ################ lpips\n",
    "    lpips_model = LPIPS(net_type='alex', device=device).to('cpu')\n",
    "    lpips_model.eval()\n",
    "    image_size = 256\n",
    "    #train and val datasets must be given to the dataset module dict. Hence, we have provided a dummy instance for both module dicts.\n",
    "    #Note that they are not used for calculating the metrics\n",
    "    dataset_module_gt = dict(\n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=\"\", image_dir_lr=\"\", downsample_factor=4,mirror_augment_prob=0), #dummy\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=\"\", image_dir_lr=\"\"), #dummy\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=gt_path, image_dir_lr=gt_path),\n",
    "    )\n",
    "    \n",
    "    dataset_module_gt = DatasetModule(**dataset_module_gt)\n",
    "    dataset_module_gt.setup('test')\n",
    "    first_dataloader = dataset_module_gt.test_dataloader()\n",
    "    \n",
    "    #train and val datasets must be given to the dataset module dict. Hence, we have provided a dummy instance for both module dicts.\n",
    "    #Note that they are not used for calculating the metrics\n",
    "    dataset_module_sr = dict( \n",
    "        num_workers=4,\n",
    "        train_batch_size=train_batch_size,\n",
    "        val_batch_size=val_batch_size,\n",
    "        test_batch_size=test_batch_size,\n",
    "        train_dataset_config=dict(image_size=256, image_dir_hr=\"\", image_dir_lr=\"\", downsample_factor=4,mirror_augment_prob=0),#dummy\n",
    "        val_dataset_config=dict(image_size=256, image_dir_hr=\"\", image_dir_lr=\"\"), #dummy\n",
    "        test_dataset_config=dict(image_size=256, image_dir_hr=sr_path, image_dir_lr=sr_path),\n",
    "    )\n",
    "    \n",
    "    data_module_sr = DatasetModule(**dataset_module_sr)\n",
    "    data_module_sr.setup('test')\n",
    "    second_dataloader = data_module_sr.test_dataloader()\n",
    "    \n",
    "    def get_lpips_mean(dataloader1,dataloader2,lpips_model,device,dataset_type):\n",
    "        lpips_model.to(device)\n",
    "        lpips_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} LPIPS on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 0.5 + 0.5\n",
    "                lpips = lpips_model(gt_images, sr_images, return_similarity=True)\n",
    "                lpips_list.append(lpips.cpu())\n",
    "        lpips_list = torch.cat(lpips_list).numpy()\n",
    "        lpips_mean = np.nanmean(lpips_list)\n",
    "        lpips_model.to('cpu')\n",
    "        return lpips_mean\n",
    "    \n",
    "    \n",
    "    \n",
    "    lpips_mean = get_lpips_mean(first_dataloader,second_dataloader,lpips_model,device,\"lpips\")\n",
    "        \n",
    "    # Constants for SSIM calculation\n",
    "\n",
    "    def create_gaussian_window(size=11, sigma=1.5):\n",
    "        kernel = cv2.getGaussianKernel(size, sigma)\n",
    "        window = np.outer(kernel, kernel)\n",
    "        return window\n",
    "    \n",
    "    def compute_mean(image, window):\n",
    "        return cv2.filter2D(image, -1, window)\n",
    "    \n",
    "    def compute_variance(image, mean, window):\n",
    "        return cv2.filter2D(image ** 2, -1, window) - mean ** 2\n",
    "    \n",
    "    def compute_covariance(image1, image2, mean1, mean2, window):\n",
    "        return cv2.filter2D(image1 * image2, -1, window) - mean1 * mean2\n",
    "    \n",
    "    def ssim_fn(img1, img2):\n",
    "\n",
    "        img1 = img1.astype(np.float64)\n",
    "        img2 = img2.astype(np.float64)\n",
    "    \n",
    "        # generate Gaussian window\n",
    "        window = create_gaussian_window()\n",
    "    \n",
    "        # Compute means\n",
    "        mean1 = compute_mean(img1, window)\n",
    "        mean2 = compute_mean(img2, window)\n",
    "    \n",
    "        # Compute variances\n",
    "        variance1 = compute_variance(img1, mean1, window)\n",
    "        variance2 = compute_variance(img2, mean2, window)\n",
    "    \n",
    "        # Compute covariance\n",
    "        covariance = compute_covariance(img1, img2, mean1, mean2, window)\n",
    "    \n",
    "        # Calculate SSIM score\n",
    "        mean_product = 2 * mean1 * mean2\n",
    "        mean_sum = mean1 ** 2 + mean2 ** 2\n",
    "        variance_sum = variance1 + variance2\n",
    "        covariance_product = 2 * covariance\n",
    "    \n",
    "        numerator = (mean_product + (0.01 * 255) ** 2) * (covariance_product + (0.03 * 255) ** 2)\n",
    "        denominator = (mean_sum + (0.01 * 255) ** 2) * (variance_sum + (0.03 * 255) ** 2)\n",
    "        ssim_score = numerator / denominator\n",
    "\n",
    "        # Return mean SSIM\n",
    "        return ssim_score.mean()\n",
    "\n",
    "    def get_ssim_mean(dataloader1,dataloader2,device,dataset_type):\n",
    "        ssim_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} SSIM on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 127.5 + 127.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 127.5 + 127.5\n",
    "                gt_images = gt_images.squeeze(0).detach().cpu().numpy().transpose(1,2,0)\n",
    "                sr_images = sr_images.squeeze(0).detach().cpu().numpy().transpose(1,2,0)\n",
    "                ssim_val = ssim_fn(sr_images, gt_images)\n",
    "                ssim_list.append(ssim_val)\n",
    "        ssim_mean = np.nanmean(ssim_list)\n",
    "        return ssim_mean\n",
    "        \n",
    "    ssim_mean = get_ssim_mean(first_dataloader,second_dataloader,device,\"ssim\")\n",
    "    \n",
    "    #### PSNR\n",
    "    \n",
    "    def psnr(img1, img2, max_val=1.0):\n",
    "        # Convert images to float tensors\n",
    "        img1 = img1.float()\n",
    "        img2 = img2.float()\n",
    "        \n",
    "        max_val = img1.max()\n",
    "        # Calculate MSE (Mean Squared Error)\n",
    "        mse = F.mse_loss(img1, img2)\n",
    "        \n",
    "        # Calculate PSNR (Peak Signal-to-Noise Ratio)\n",
    "        psnr = 20 * torch.log10(max_val / torch.sqrt(mse))\n",
    "        \n",
    "        return psnr.item()\n",
    "    \n",
    "    def get_psnr_mean(dataloader1,dataloader2,device,dataset_type):\n",
    "        psnr_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch1,batch2 in tqdm(zip(dataloader1,dataloader2), desc=f\"Calculating {dataset_type} PSNR on sr images\", total=len(dataloader1)):\n",
    "                gt_images = batch1[\"image_hr\"].to(device) * 127.5 + 127.5\n",
    "                sr_images = batch2[\"image_hr\"].to(device) * 127.5 + 127.5\n",
    "                psnr_val = psnr(sr_images, gt_images)\n",
    "                psnr_list.append(psnr_val)\n",
    "        psnr_mean = np.nanmean(psnr_list)\n",
    "        return psnr_mean\n",
    "    \n",
    "    \n",
    "    psnr_mean = get_psnr_mean(first_dataloader,second_dataloader,device,\"psnr\")\n",
    "    print(f\"for the {label} scores are:\\n psnr: {psnr_mean}\\n lpips: {lpips_mean} \\n ssim: {ssim_mean}\")\n",
    "\n",
    "metric_path_dict = {\n",
    "    \"Set5\":[\"results_Set5/\", \"gt_Set5\"],\n",
    "    \"Set14\":[\"results_Set14/\", \"gt_Set14\"],\n",
    "    \"urban100\":[\"results_urban100/\", \"gt_urban100\"],\n",
    "    \"manga109\":[\"results_manga/\", \"gt_manga\"],\n",
    "    \"DIV2K\": [\"results_DIV2K_valid_HR/\", \"gt_div2k\"]\n",
    "}\n",
    "\n",
    "models_list = [\"patchgan/\", \"patchgan_sed/\", \"pixelwise/\", \"pixelwise_sed/\"]\n",
    "\n",
    "for dataset, dir_ in metric_path_dict.items():\n",
    "    for model in models_list:\n",
    "        sr_path = dir_[0] + model\n",
    "        gt_path = dir_[1] + \"/\"\n",
    "        label = dataset + \" using \" + model.replace(\"/\",\"\")\n",
    "        print_metrics_given_path(sr_path, gt_path, label, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "### Quantitative Comparisions\n",
    "\n",
    "| Dataset  | Method          | PSNR (Ours)             | PSNR              | LPIPS     (Ours)        | LPIPS             | SSIM    (Ours)          | SSIM              |\n",
    "|----------|-----------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n",
    "| Set5     | patchgan        | 28.3              | 30.6              | 0.073             | 0.070             | 0.820             | 0.860             |\n",
    "| Set5     | patchgan_sed    | 28.5              | 31.2              | 0.077             | 0.064             | 0.827             | 0.867             |\n",
    "| Set5     | pixelwise       | 28.9              | 31.1              | 0.111             | 0.072             | 0.842             | 0.869             |\n",
    "| Set5     | pixelwise_sed   | 28.6              | 31.7              | 0.088             | 0.069             | 0.827             | 0.880             |\n",
    "| Set14    | patchgan        | 25.4              | 26.9              | 0.127             | 0.130             | 0.705             | 0.724             |\n",
    "| Set14    | patchgan_sed    | 25.5              | 27.3              | 0.127             | 0.117             | 0.712             | 0.736             |\n",
    "| Set14    | pixelwise       | 25.9              | 27.5              | 0.181             | 0.127             | 0.733             | 0.739             |\n",
    "| Set14    | pixelwise_sed   | 25.6              | 27.9              | 0.148             | 0.123             | 0.717             | 0.757             |\n",
    "| Urban100 | patchgan        | 23.4              | 24.8              | 0.133             | 0.120             | 0.704             | 0.752             |\n",
    "| Urban100 | patchgan_sed    | 23.4              | 25.9              | 0.135             | 0.106             | 0.707             | 0.779             |\n",
    "| Urban100 | pixelwise       | 23.6              | 25.6              | 0.178             | 0.125             | 0.721             | 0.768             |\n",
    "| Urban100 | pixelwise_sed   | 23.4              | 26.2              | 0.153             | 0.112             | 0.711             | 0.788             |\n",
    "| Manga109 | patchgan        | 26.6              | 28.6              | 0.055             | 0.058             | 0.824             | 0.872             |\n",
    "| Manga109 | patchgan_sed    | 26.5              | 29.9              | 0.055             | 0.048             | 0.823             | 0.888             |\n",
    "| Manga109 | pixelwise       | 26.9              | 29.4              | 0.069             | 0.056             | 0.836             | 0.882             |\n",
    "| Manga109 | pixelwise_sed   | 26.7              | 30.4              | 0.061             | 0.047             | 0.828             | 0.897             |\n",
    "| DIV2K    | patchgan        | 31.4              | 28.7              | 0.082             | 0.111             | 0.831             | 0.792             |\n",
    "| DIV2K    | patchgan_sed    | 31.5              | 29.2              | 0.084             | 0.094             | 0.831             | 0.802             |\n",
    "| DIV2K    | pixelwise       | 31.4              | 29.2              | 0.123             | 0.110             | 0.846             | 0.802             |\n",
    "| DIV2K    | pixelwise_sed   | 31.7              | 29.9              | 0.103             | 0.102             | 0.836             | 0.818             |\n",
    "\n",
    "## Qualitative Comparison for our goals\n",
    "**Ground Truth (GT) - GT downscaled by 4 - Patchgan - Patchgan + SeD - Pixelwise - Pixelwise + SeD**\n",
    "<img src=\"img/merged_image2.jpg\" /> <br/>\n",
    "<img src=\"img/merged_image3.jpg\" /> <br/>\n",
    "<img src=\"img/merged_image4.jpg\" /> <br/>\n",
    "<img src=\"img/merged_image5.jpg\" /> <br/>\n",
    "<img src=\"img/merged_image6.jpg\" /> <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges we have encountered when implementing the paper\n",
    "- The main challenge was that there were too many ambiguities in the paper (dimension of layers, multi-head/single head attention)\n",
    "- Preprocessing the data was also a challenge as the authors did not specify how they have preprocessed the data so we have decided to go with cropping the datasets to enhance the size of the dataset\n",
    "- Implementing CLIP feature extractor was also a challenge while trying to get the outputs from the 3rd layer.\n",
    "- Integrating the Semantic Aware Fusion Block was also a challenge because there were dimensionality mismatches. At first, we have tried to implement the model with the given setup but we have encountered dimensionality mismatches. As a result, we have decided to add extra convolution layers to make the dimensions compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comments and Responses\n",
    "\n",
    "#### Comment: \n",
    "**Paper author information is not provided.**\n",
    "\n",
    "**Response:**\n",
    "Provided in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**We could not try the notebook due to the problems on downloading datasets (big size and low download speed), one restricted download link and path issues (elaborated below).**\n",
    "\n",
    "**Response:**\n",
    "Download problems resolved (for restriction and issues). We also faced the big size in some servers but were able to download on another server.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Because of the technical issues they mentioned to the instructor, the declared plots are for a small subset of the dataset.**\n",
    "\n",
    "**Response:**\n",
    "We also demonstrate training logic on a small toy dataset (as we cannot keep the Jupyter notebook open for 1.8 days). However, declared plots are for original results.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Download script is problematic for the reasons stated in the additional comments part below.**\n",
    "\n",
    "**Response:**\n",
    "Path problems, etc., fixed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Why single head cross attention is used instead of multi-head cross attention? (Why num_heads is 1?)**\n",
    "\n",
    "**Response:**\n",
    "The paper does not explicitly mention that they use multi-head cross attention. Hence, we used a single head cross attention logic. But this can still be adjusted in the code for multi-head easily.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**It is not written in the assumptions, but the feature maps are assumed to have a dimension of 128, which we cannot see in the paper explicitly. Is there a reason for this? (Why “dim_head” is 128?)**\n",
    "\n",
    "**Response:**\n",
    "For the computational resources we have, a dimension of 128 was the maximum that we could handle. The paper also does not explicitly mention dimensionality. This can be changed for larger GPU resources easily.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Is the choice of lambda values in the loss as 1 and 10 stems from a reason? For example, is it derived by hyperparameter experiments? We think this choice should affect the training significantly and the reason for lambda choices should have a reasoning.**\n",
    "\n",
    "**Response:**\n",
    "We conducted hyperparameter tuning with a few experiments. These values reproduced the original paper results (almost). Our main focus was to make loss values (Perceptual, VGG, Adversarial) on the same scale.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Because of the technical problems they have encountered, they could not measure the metrics on the desired datasets.**\n",
    "\n",
    "**Response:**\n",
    "Metrics are measured in v2, and target results are achieved.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**The notebook also reports the original values of the targeted quantitative results, for comparison. They are not explicitly stated in the notebook.**\n",
    "\n",
    "**Response:**\n",
    "They are added in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**LICENSE contains only Yiğit Ekin’s name.**\n",
    "\n",
    "**Response:**\n",
    "Fixed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**For RRDB module, leaky relu (or any activation) is not used between conv layers, which is contrary to the referred RRDB paper.**\n",
    "\n",
    "**Response:**\n",
    "Very good catch. We thank the reviewers. Fixed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Plots and examples are demonstrated well. We think that adding the `train.py` files content to the main Jupyter notebook is not necessary.**\n",
    "\n",
    "**Response:**\n",
    "Since they are needed in the demo, we hold them as well in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**The code lacks comments, which makes it hard to follow the code. It should contain more comments, possibly referring where it is related to in which paper for easier interpretability.**\n",
    "\n",
    "**Response:**\n",
    "More comments are added in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**Using too much (e.g., `RRDBNet` -> `Residual_in_ResidualBlock` -> `DenseBlock` -> `make_blocks` -> `get_layer` -> `Conv2d`) hierarchy deteriorates the code’s readability in our opinion. It may be 1-2 levels more basic to enhance following up the code.**\n",
    "\n",
    "**Response:**\n",
    "We did not change this part, as we build these blocks based on the referred paper.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**In the implementations `models/patchgan_discriminator.py` file, there is an absolute path import `sys.path.append('/scratch/users/hpc-yekin/hpc_run/SeD/models')`, which should be changed with a dynamic path like `sys.path.append('./models')`, since the former can only work with your local PC, and the latter may be more flexible.**\n",
    "\n",
    "**Response:**\n",
    "Fixed in v2, now using dynamic path.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**The `environment.yml` file does not install `clip`, but it must be imported in `models/feature_extractor_model.py`.**\n",
    "\n",
    "**Response:**\n",
    "Clip added to `environment.yml` in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**`download_dataset.sh` did not work with your `resize_4.py`. This is because the parser should have `--folder` instead of `folder` and `--save_path` instead of `save_path` (-- is missing). The `gdown https://drive.google.com/file/d/1henrktM4Cw9hJIJBDEObAzl-eCbpzNaJ/view?usp=drive_link` part does not work since the link requires access. Moreover, the download of `Flickr2K` was very slow (lower than 500kb/s) in our case. Again, it may be a problem on our side, but it is likely to be caused by the enabled download rate from the given link. If some other faster link or option is provided in the script, it would be much better.**\n",
    "\n",
    "**Response:**\n",
    "Fixed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**In the `prepare_dataset.py` script, the `os.mkdir(\"data2/dataset_cropped\") if not os.path.exists(\"data2/dataset_cropped\") else None` line does not work. You can replace it with `os.makedirs(\"data2/dataset_cropped\", exist_ok=True)`. Similarly, replacing the `os.mkdir(save_folder) if not os.path.exists(save_folder) else None` with `os.makedirs(save_folder, exist_ok=True)` should work.**\n",
    "\n",
    "**Response:**\n",
    "Fixed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**The script `prepare_dataset.py` does not work since it assumes there is a `data2/hr` and `data2/lr` directories exist already. (we did not understand the `data2` folder. Maybe, it is left from the code authors’ local environment)**\n",
    "\n",
    "**Response:**\n",
    "It was used for a toy dataset. Removed in v2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**In the `models/super_resolution_module.py` there is a possible redundant model instance creation at line 34. It should be merged with an if-else block with line 36. Creating a `PatchDiscriminatorWithSeD` instance can be somehow costly and if `is_pixelwise_disc` is False, it will be created and then deleted for no reason.**\n",
    "\n",
    "**Response:**\n",
    "Fixed according to the feedback.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**In `models/super_resolution_module.py`, lines 132 and 133 send low and high-resolution batch parts to GPU, which remains unused because `get_loss_forward_dict` method uses the batch object. This causes unnecessary usage of GPU memory, which may limit the highest possible training batch size to 50%.**\n",
    "\n",
    "**Response:**\n",
    "Fixed according to the feedback.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**For SeD patchwise discriminator, the adversarial loss of the generator converged to a very high value too early, whereas the discriminator's adversarial loss converged to a low value. This may be a sign of mode collapse or unstable training (since the generator's loss always increased). Similarly, for patchwise discriminator, the generator's adversarial loss converges to a value that does not show significant improvement (decrease), whereas the discriminator's adversarial loss seems to converge to a low value too early, may be caused by the same issues.**\n",
    "\n",
    "**Response:**\n",
    "We do not find this review very useful (and also not intuitive). We detected that the main problem in our v1 submission was related to a data augmentation problem in our dataloader class. We were applying mirror augmentation to high resolution images and not to low resolution images during training. As a result, the generator was not able to learn the correct mapping between low and high resolution images. To be precise, it is not included in our logs but the generator of v1 was learning to create a blurry image where the instance on one side was copied on to the other side (like a model that duplicates the object on the right side of the image to both left and right side). We solved the dataloader problem, and discriminator and generator losses started looking as desired.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comment: \n",
    "**In the discriminator's initial part, the paper only states a “conv” in the architecture. However, using a 4-block downsampler with stride=2 may cause an imbalance between the discriminator and the generator. One of the reasons the discriminator learns too much and the generator learns nearly nothing might be due to this (reported plots of training adversarial losses in the notebook). We are suggesting paying attention to discriminator experiments.**\n",
    "\n",
    "**Response:**\n",
    "We do not find this review very useful (and also not intuitive). As mentioned before, we detected that the main problem in our v1 submission was related to a data augmentation problem in our dataloader class. We solved the dataloader problem, and discriminator and generator losses started looking as desired.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
